{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t9r5GO3WapuP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/human_activity_recognition.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "elAfepGB3NrG",
        "outputId": "2e2c45df-15f9-4bce-a83c-28f97842fe94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Column 1  Column 2  Column 3  Column 4  Column 5  Column 6  Column 7  \\\n",
              "0   0.69755  0.282300   0.76965   1.22910   1.23540   0.70616   0.88037   \n",
              "1   0.18631  0.094392   0.25299   0.44290   0.94970   0.63257   0.53620   \n",
              "2   0.22088  0.139700   0.54641   0.69633   0.72497   1.00800   0.65949   \n",
              "3   0.71579  0.368160   0.72796   0.22885   1.02090   1.43060   0.44613   \n",
              "4   1.25620  0.649310   1.07830   1.30660   1.35680   2.57880   0.96700   \n",
              "\n",
              "   Column 8  Column 9  Column 10  ...  Unnamed: 8993  Unnamed: 8994  \\\n",
              "0   1.30680   1.21920    0.18614  ...        1.49340       1.643400   \n",
              "1   0.67392   0.66807    1.10160  ...        0.25774       0.378330   \n",
              "2   0.27952   0.58074    0.68376  ...        0.36091       0.674700   \n",
              "3   0.93522   0.92986    0.43799  ...        0.12124       0.927550   \n",
              "4   1.01730   0.66294    1.54360  ...        0.72408       0.070177   \n",
              "\n",
              "   Unnamed: 8995  Unnamed: 8996  Unnamed: 8997  Unnamed: 8998  Unnamed: 8999  \\\n",
              "0        1.69100        2.64540        3.60610        3.80920       3.374900   \n",
              "1        0.43021        0.58438        0.41642        0.14651       0.286560   \n",
              "2        0.50030        0.86020        0.50468        0.28592       0.327720   \n",
              "3        0.44305        1.75210        0.73253        0.57487       0.040405   \n",
              "4        0.52158        1.28860        1.23130        1.36580       1.600600   \n",
              "\n",
              "   Activity  Unnamed: 9001  Unnamed: 9002  \n",
              "0         0           1500              1  \n",
              "1         0           1500              2  \n",
              "2         0           1500              3  \n",
              "3         0           1500              4  \n",
              "4         0           1500              5  \n",
              "\n",
              "[5 rows x 9003 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-995451b6-fe51-4398-954b-0da9bd723efd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column 1</th>\n",
              "      <th>Column 2</th>\n",
              "      <th>Column 3</th>\n",
              "      <th>Column 4</th>\n",
              "      <th>Column 5</th>\n",
              "      <th>Column 6</th>\n",
              "      <th>Column 7</th>\n",
              "      <th>Column 8</th>\n",
              "      <th>Column 9</th>\n",
              "      <th>Column 10</th>\n",
              "      <th>...</th>\n",
              "      <th>Unnamed: 8993</th>\n",
              "      <th>Unnamed: 8994</th>\n",
              "      <th>Unnamed: 8995</th>\n",
              "      <th>Unnamed: 8996</th>\n",
              "      <th>Unnamed: 8997</th>\n",
              "      <th>Unnamed: 8998</th>\n",
              "      <th>Unnamed: 8999</th>\n",
              "      <th>Activity</th>\n",
              "      <th>Unnamed: 9001</th>\n",
              "      <th>Unnamed: 9002</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.69755</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>0.76965</td>\n",
              "      <td>1.22910</td>\n",
              "      <td>1.23540</td>\n",
              "      <td>0.70616</td>\n",
              "      <td>0.88037</td>\n",
              "      <td>1.30680</td>\n",
              "      <td>1.21920</td>\n",
              "      <td>0.18614</td>\n",
              "      <td>...</td>\n",
              "      <td>1.49340</td>\n",
              "      <td>1.643400</td>\n",
              "      <td>1.69100</td>\n",
              "      <td>2.64540</td>\n",
              "      <td>3.60610</td>\n",
              "      <td>3.80920</td>\n",
              "      <td>3.374900</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.18631</td>\n",
              "      <td>0.094392</td>\n",
              "      <td>0.25299</td>\n",
              "      <td>0.44290</td>\n",
              "      <td>0.94970</td>\n",
              "      <td>0.63257</td>\n",
              "      <td>0.53620</td>\n",
              "      <td>0.67392</td>\n",
              "      <td>0.66807</td>\n",
              "      <td>1.10160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25774</td>\n",
              "      <td>0.378330</td>\n",
              "      <td>0.43021</td>\n",
              "      <td>0.58438</td>\n",
              "      <td>0.41642</td>\n",
              "      <td>0.14651</td>\n",
              "      <td>0.286560</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.22088</td>\n",
              "      <td>0.139700</td>\n",
              "      <td>0.54641</td>\n",
              "      <td>0.69633</td>\n",
              "      <td>0.72497</td>\n",
              "      <td>1.00800</td>\n",
              "      <td>0.65949</td>\n",
              "      <td>0.27952</td>\n",
              "      <td>0.58074</td>\n",
              "      <td>0.68376</td>\n",
              "      <td>...</td>\n",
              "      <td>0.36091</td>\n",
              "      <td>0.674700</td>\n",
              "      <td>0.50030</td>\n",
              "      <td>0.86020</td>\n",
              "      <td>0.50468</td>\n",
              "      <td>0.28592</td>\n",
              "      <td>0.327720</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.71579</td>\n",
              "      <td>0.368160</td>\n",
              "      <td>0.72796</td>\n",
              "      <td>0.22885</td>\n",
              "      <td>1.02090</td>\n",
              "      <td>1.43060</td>\n",
              "      <td>0.44613</td>\n",
              "      <td>0.93522</td>\n",
              "      <td>0.92986</td>\n",
              "      <td>0.43799</td>\n",
              "      <td>...</td>\n",
              "      <td>0.12124</td>\n",
              "      <td>0.927550</td>\n",
              "      <td>0.44305</td>\n",
              "      <td>1.75210</td>\n",
              "      <td>0.73253</td>\n",
              "      <td>0.57487</td>\n",
              "      <td>0.040405</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.25620</td>\n",
              "      <td>0.649310</td>\n",
              "      <td>1.07830</td>\n",
              "      <td>1.30660</td>\n",
              "      <td>1.35680</td>\n",
              "      <td>2.57880</td>\n",
              "      <td>0.96700</td>\n",
              "      <td>1.01730</td>\n",
              "      <td>0.66294</td>\n",
              "      <td>1.54360</td>\n",
              "      <td>...</td>\n",
              "      <td>0.72408</td>\n",
              "      <td>0.070177</td>\n",
              "      <td>0.52158</td>\n",
              "      <td>1.28860</td>\n",
              "      <td>1.23130</td>\n",
              "      <td>1.36580</td>\n",
              "      <td>1.600600</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 9003 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-995451b6-fe51-4398-954b-0da9bd723efd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-995451b6-fe51-4398-954b-0da9bd723efd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-995451b6-fe51-4398-954b-0da9bd723efd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data by splitting it into features and target variables\n",
        "X = df.drop([\"Activity\",\"Unnamed: 9001\",\"Unnamed: 9002\"], axis=1)\n",
        "y = df[\"Activity\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=30)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=31)"
      ],
      "metadata": {
        "id": "svOF1U8KPfi5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the random forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=32)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_val)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy: {:.2f}%\".format(acc * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XnlOgafKEBd",
        "outputId": "e0bac513-95e5-4d4b-869b-6b93c079ef07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.fit_transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "wAi6u7-5oVDO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(2**10, activation=\"sigmoid\"))\n",
        "model.add(Dense(2**10, activation=\"sigmoid\"))\n",
        "model.add(Dense(2**5, activation=\"sigmoid\"))\n",
        "model.add(Dense(18, activation=\"softmax\"))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=500,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff7QgWQKGKGO",
        "outputId": "d3bac50d-353a-4770-d789-3eaae40a1c77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 11s 771ms/step - loss: 2.6602 - accuracy: 0.1343 - val_loss: 2.4409 - val_accuracy: 0.2184\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 9s 563ms/step - loss: 2.3462 - accuracy: 0.2924 - val_loss: 2.2053 - val_accuracy: 0.3942\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 10s 746ms/step - loss: 2.0780 - accuracy: 0.4975 - val_loss: 1.9369 - val_accuracy: 0.5887\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 7s 466ms/step - loss: 1.8238 - accuracy: 0.6687 - val_loss: 1.7093 - val_accuracy: 0.6561\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 9s 636ms/step - loss: 1.5892 - accuracy: 0.7249 - val_loss: 1.4715 - val_accuracy: 0.7321\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 7s 472ms/step - loss: 1.3407 - accuracy: 0.7680 - val_loss: 1.2533 - val_accuracy: 0.7517\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 9s 627ms/step - loss: 1.1380 - accuracy: 0.8075 - val_loss: 1.0816 - val_accuracy: 0.8294\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 7s 466ms/step - loss: 0.9865 - accuracy: 0.8573 - val_loss: 0.9466 - val_accuracy: 0.8865\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 9s 624ms/step - loss: 0.8649 - accuracy: 0.8752 - val_loss: 0.8450 - val_accuracy: 0.8891\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 7s 472ms/step - loss: 0.7682 - accuracy: 0.8925 - val_loss: 0.7601 - val_accuracy: 0.9036\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b7bbc1190>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(2**10, activation=\"relu\"))\n",
        "model.add(Dense(2**10, activation=\"relu\"))\n",
        "model.add(Dense(2**5, activation=\"relu\"))\n",
        "model.add(Dense(18, activation=\"softmax\"))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=500,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjFGNqm9GKD6",
        "outputId": "eafb3e43-7ea2-498d-d1fe-d8d8f7119237"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 10s 651ms/step - loss: 3.7263 - accuracy: 0.1418 - val_loss: 2.7031 - val_accuracy: 0.1468\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 7s 472ms/step - loss: 2.6605 - accuracy: 0.1616 - val_loss: 2.6833 - val_accuracy: 0.1621\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 9s 616ms/step - loss: 2.5633 - accuracy: 0.1783 - val_loss: 2.5317 - val_accuracy: 0.1886\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 7s 486ms/step - loss: 2.4209 - accuracy: 0.2350 - val_loss: 2.2885 - val_accuracy: 0.3166\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 8s 589ms/step - loss: 2.4146 - accuracy: 0.2846 - val_loss: 2.3311 - val_accuracy: 0.2645\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 7s 480ms/step - loss: 2.2656 - accuracy: 0.2787 - val_loss: 2.1696 - val_accuracy: 0.2355\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 8s 584ms/step - loss: 2.3634 - accuracy: 0.2990 - val_loss: 2.2128 - val_accuracy: 0.3072\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 7s 509ms/step - loss: 2.1875 - accuracy: 0.3124 - val_loss: 2.1633 - val_accuracy: 0.3038\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 9s 607ms/step - loss: 2.1355 - accuracy: 0.3139 - val_loss: 2.1212 - val_accuracy: 0.3174\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 7s 511ms/step - loss: 2.0504 - accuracy: 0.3387 - val_loss: 2.0256 - val_accuracy: 0.3370\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b7c9285b0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(2**10, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**10, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**7, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**5, activation=\"sigmoid\"))\n",
        "model.add(Dense(18, activation=\"softmax\"))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=200,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_99H595GKBb",
        "outputId": "029df944-ec79-4eb8-d9a5-f2743e34bf4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "34/34 [==============================] - 10s 263ms/step - loss: 2.6727 - accuracy: 0.1650 - val_loss: 2.4068 - val_accuracy: 0.2440\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 11s 311ms/step - loss: 2.2512 - accuracy: 0.2867 - val_loss: 2.0113 - val_accuracy: 0.4249\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 11s 312ms/step - loss: 1.8781 - accuracy: 0.4894 - val_loss: 1.6629 - val_accuracy: 0.5981\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 9s 255ms/step - loss: 1.6014 - accuracy: 0.6127 - val_loss: 1.4359 - val_accuracy: 0.6681\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 11s 311ms/step - loss: 1.4057 - accuracy: 0.6512 - val_loss: 1.2661 - val_accuracy: 0.7014\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 11s 313ms/step - loss: 1.2480 - accuracy: 0.7023 - val_loss: 1.1226 - val_accuracy: 0.7509\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 9s 254ms/step - loss: 1.1165 - accuracy: 0.7495 - val_loss: 0.9972 - val_accuracy: 0.7500\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 11s 311ms/step - loss: 0.9962 - accuracy: 0.7691 - val_loss: 0.9036 - val_accuracy: 0.7560\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 11s 311ms/step - loss: 0.9042 - accuracy: 0.7733 - val_loss: 0.8067 - val_accuracy: 0.7662\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 9s 255ms/step - loss: 0.8131 - accuracy: 0.7934 - val_loss: 0.7291 - val_accuracy: 0.7824\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 11s 311ms/step - loss: 0.7462 - accuracy: 0.8006 - val_loss: 0.6730 - val_accuracy: 0.7858\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 11s 313ms/step - loss: 0.6855 - accuracy: 0.8020 - val_loss: 0.6404 - val_accuracy: 0.7773\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 9s 257ms/step - loss: 0.6416 - accuracy: 0.8124 - val_loss: 0.5992 - val_accuracy: 0.7773\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 11s 314ms/step - loss: 0.6054 - accuracy: 0.8122 - val_loss: 0.5572 - val_accuracy: 0.7892\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 11s 313ms/step - loss: 0.5684 - accuracy: 0.8155 - val_loss: 0.5303 - val_accuracy: 0.7858\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 9s 257ms/step - loss: 0.5335 - accuracy: 0.8213 - val_loss: 0.5040 - val_accuracy: 0.7875\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 11s 314ms/step - loss: 0.5113 - accuracy: 0.8200 - val_loss: 0.4882 - val_accuracy: 0.7892\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 10s 310ms/step - loss: 0.4995 - accuracy: 0.8142 - val_loss: 0.4784 - val_accuracy: 0.7875\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 9s 254ms/step - loss: 0.4689 - accuracy: 0.8184 - val_loss: 0.4700 - val_accuracy: 0.7833\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 10s 310ms/step - loss: 0.4548 - accuracy: 0.8244 - val_loss: 0.4447 - val_accuracy: 0.7884\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b8b220dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "# model.add(Dense(2**9, activation=\"sigmoid\"))\n",
        "# model.add(Dropout(.4))\n",
        "model.add(Dense(2**9, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**7, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**5, activation=\"sigmoid\"))\n",
        "model.add(Dense(18, activation=\"softmax\"))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=200,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHtAPwmZGJ--",
        "outputId": "b6064a4c-ee67-4aa1-f3b7-ea3121eac765"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "34/34 [==============================] - 8s 211ms/step - loss: 2.6063 - accuracy: 0.1913 - val_loss: 2.3052 - val_accuracy: 0.3584\n",
            "Epoch 2/100\n",
            "34/34 [==============================] - 4s 106ms/step - loss: 2.1593 - accuracy: 0.4213 - val_loss: 1.9203 - val_accuracy: 0.5853\n",
            "Epoch 3/100\n",
            "34/34 [==============================] - 3s 103ms/step - loss: 1.8211 - accuracy: 0.5926 - val_loss: 1.6082 - val_accuracy: 0.6519\n",
            "Epoch 4/100\n",
            "34/34 [==============================] - 5s 143ms/step - loss: 1.5319 - accuracy: 0.7079 - val_loss: 1.3382 - val_accuracy: 0.7304\n",
            "Epoch 5/100\n",
            "34/34 [==============================] - 4s 126ms/step - loss: 1.2906 - accuracy: 0.7735 - val_loss: 1.1204 - val_accuracy: 0.7645\n",
            "Epoch 6/100\n",
            "34/34 [==============================] - 3s 103ms/step - loss: 1.0909 - accuracy: 0.8002 - val_loss: 0.9490 - val_accuracy: 0.7671\n",
            "Epoch 7/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.9355 - accuracy: 0.8193 - val_loss: 0.8234 - val_accuracy: 0.7696\n",
            "Epoch 8/100\n",
            "34/34 [==============================] - 5s 157ms/step - loss: 0.8129 - accuracy: 0.8371 - val_loss: 0.7105 - val_accuracy: 0.8072\n",
            "Epoch 9/100\n",
            "34/34 [==============================] - 4s 110ms/step - loss: 0.7157 - accuracy: 0.8473 - val_loss: 0.6262 - val_accuracy: 0.8720\n",
            "Epoch 10/100\n",
            "34/34 [==============================] - 4s 103ms/step - loss: 0.6347 - accuracy: 0.8606 - val_loss: 0.5689 - val_accuracy: 0.9053\n",
            "Epoch 11/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.5727 - accuracy: 0.8713 - val_loss: 0.4983 - val_accuracy: 0.9164\n",
            "Epoch 12/100\n",
            "34/34 [==============================] - 6s 164ms/step - loss: 0.5107 - accuracy: 0.8966 - val_loss: 0.4473 - val_accuracy: 0.9224\n",
            "Epoch 13/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.4635 - accuracy: 0.9090 - val_loss: 0.3925 - val_accuracy: 0.9565\n",
            "Epoch 14/100\n",
            "34/34 [==============================] - 3s 103ms/step - loss: 0.4179 - accuracy: 0.9257 - val_loss: 0.3542 - val_accuracy: 0.9590\n",
            "Epoch 15/100\n",
            "34/34 [==============================] - 4s 114ms/step - loss: 0.3769 - accuracy: 0.9387 - val_loss: 0.3272 - val_accuracy: 0.9497\n",
            "Epoch 16/100\n",
            "34/34 [==============================] - 5s 154ms/step - loss: 0.3560 - accuracy: 0.9388 - val_loss: 0.2941 - val_accuracy: 0.9659\n",
            "Epoch 17/100\n",
            "34/34 [==============================] - 4s 103ms/step - loss: 0.3346 - accuracy: 0.9361 - val_loss: 0.2873 - val_accuracy: 0.9531\n",
            "Epoch 18/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.3044 - accuracy: 0.9471 - val_loss: 0.2547 - val_accuracy: 0.9701\n",
            "Epoch 19/100\n",
            "34/34 [==============================] - 4s 119ms/step - loss: 0.2856 - accuracy: 0.9489 - val_loss: 0.2643 - val_accuracy: 0.9078\n",
            "Epoch 20/100\n",
            "34/34 [==============================] - 5s 141ms/step - loss: 0.2982 - accuracy: 0.9251 - val_loss: 0.2311 - val_accuracy: 0.9829\n",
            "Epoch 21/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.3038 - accuracy: 0.9209 - val_loss: 0.2325 - val_accuracy: 0.9778\n",
            "Epoch 22/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.2536 - accuracy: 0.9480 - val_loss: 0.2035 - val_accuracy: 0.9795\n",
            "Epoch 23/100\n",
            "34/34 [==============================] - 4s 131ms/step - loss: 0.2329 - accuracy: 0.9552 - val_loss: 0.1909 - val_accuracy: 0.9846\n",
            "Epoch 24/100\n",
            "34/34 [==============================] - 5s 134ms/step - loss: 0.2209 - accuracy: 0.9581 - val_loss: 0.1662 - val_accuracy: 0.9855\n",
            "Epoch 25/100\n",
            "34/34 [==============================] - 3s 100ms/step - loss: 0.2077 - accuracy: 0.9616 - val_loss: 0.1688 - val_accuracy: 0.9778\n",
            "Epoch 26/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.1933 - accuracy: 0.9664 - val_loss: 0.1524 - val_accuracy: 0.9838\n",
            "Epoch 27/100\n",
            "34/34 [==============================] - 5s 145ms/step - loss: 0.1793 - accuracy: 0.9694 - val_loss: 0.1496 - val_accuracy: 0.9881\n",
            "Epoch 28/100\n",
            "34/34 [==============================] - 4s 120ms/step - loss: 0.1794 - accuracy: 0.9641 - val_loss: 0.1392 - val_accuracy: 0.9821\n",
            "Epoch 29/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1831 - accuracy: 0.9581 - val_loss: 0.1902 - val_accuracy: 0.9172\n",
            "Epoch 30/100\n",
            "34/34 [==============================] - 3s 103ms/step - loss: 0.1737 - accuracy: 0.9610 - val_loss: 0.1415 - val_accuracy: 0.9846\n",
            "Epoch 31/100\n",
            "34/34 [==============================] - 5s 158ms/step - loss: 0.1784 - accuracy: 0.9560 - val_loss: 0.1215 - val_accuracy: 0.9889\n",
            "Epoch 32/100\n",
            "34/34 [==============================] - 4s 110ms/step - loss: 0.1469 - accuracy: 0.9735 - val_loss: 0.1061 - val_accuracy: 0.9881\n",
            "Epoch 33/100\n",
            "34/34 [==============================] - 4s 104ms/step - loss: 0.1372 - accuracy: 0.9762 - val_loss: 0.1051 - val_accuracy: 0.9863\n",
            "Epoch 34/100\n",
            "34/34 [==============================] - 4s 105ms/step - loss: 0.1459 - accuracy: 0.9673 - val_loss: 0.1052 - val_accuracy: 0.9872\n",
            "Epoch 35/100\n",
            "34/34 [==============================] - 5s 160ms/step - loss: 0.1362 - accuracy: 0.9720 - val_loss: 0.0930 - val_accuracy: 0.9906\n",
            "Epoch 36/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1310 - accuracy: 0.9712 - val_loss: 0.0939 - val_accuracy: 0.9881\n",
            "Epoch 37/100\n",
            "34/34 [==============================] - 3s 103ms/step - loss: 0.1267 - accuracy: 0.9702 - val_loss: 0.1049 - val_accuracy: 0.9872\n",
            "Epoch 38/100\n",
            "34/34 [==============================] - 4s 118ms/step - loss: 0.1213 - accuracy: 0.9759 - val_loss: 0.1016 - val_accuracy: 0.9863\n",
            "Epoch 39/100\n",
            "34/34 [==============================] - 5s 146ms/step - loss: 0.1320 - accuracy: 0.9680 - val_loss: 0.0929 - val_accuracy: 0.9855\n",
            "Epoch 40/100\n",
            "34/34 [==============================] - 3s 98ms/step - loss: 0.1304 - accuracy: 0.9685 - val_loss: 0.1171 - val_accuracy: 0.9838\n",
            "Epoch 41/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.1325 - accuracy: 0.9671 - val_loss: 0.0935 - val_accuracy: 0.9872\n",
            "Epoch 42/100\n",
            "34/34 [==============================] - 4s 120ms/step - loss: 0.1661 - accuracy: 0.9465 - val_loss: 0.1253 - val_accuracy: 0.9838\n",
            "Epoch 43/100\n",
            "34/34 [==============================] - 5s 143ms/step - loss: 0.1811 - accuracy: 0.9385 - val_loss: 0.1160 - val_accuracy: 0.9889\n",
            "Epoch 44/100\n",
            "34/34 [==============================] - 3s 103ms/step - loss: 0.1587 - accuracy: 0.9498 - val_loss: 0.1134 - val_accuracy: 0.9838\n",
            "Epoch 45/100\n",
            "34/34 [==============================] - 3s 100ms/step - loss: 0.1572 - accuracy: 0.9518 - val_loss: 0.1070 - val_accuracy: 0.9829\n",
            "Epoch 46/100\n",
            "34/34 [==============================] - 4s 128ms/step - loss: 0.1350 - accuracy: 0.9623 - val_loss: 0.0947 - val_accuracy: 0.9821\n",
            "Epoch 47/100\n",
            "34/34 [==============================] - 5s 137ms/step - loss: 0.1362 - accuracy: 0.9646 - val_loss: 0.0964 - val_accuracy: 0.9838\n",
            "Epoch 48/100\n",
            "34/34 [==============================] - 3s 100ms/step - loss: 0.1283 - accuracy: 0.9656 - val_loss: 0.0858 - val_accuracy: 0.9881\n",
            "Epoch 49/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1181 - accuracy: 0.9696 - val_loss: 0.0822 - val_accuracy: 0.9855\n",
            "Epoch 50/100\n",
            "34/34 [==============================] - 5s 141ms/step - loss: 0.1107 - accuracy: 0.9727 - val_loss: 0.0770 - val_accuracy: 0.9881\n",
            "Epoch 51/100\n",
            "34/34 [==============================] - 4s 122ms/step - loss: 0.1032 - accuracy: 0.9730 - val_loss: 0.0836 - val_accuracy: 0.9846\n",
            "Epoch 52/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1003 - accuracy: 0.9742 - val_loss: 0.0702 - val_accuracy: 0.9872\n",
            "Epoch 53/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.1013 - accuracy: 0.9769 - val_loss: 0.0684 - val_accuracy: 0.9881\n",
            "Epoch 54/100\n",
            "34/34 [==============================] - 5s 155ms/step - loss: 0.1049 - accuracy: 0.9723 - val_loss: 0.0774 - val_accuracy: 0.9898\n",
            "Epoch 55/100\n",
            "34/34 [==============================] - 4s 109ms/step - loss: 0.1222 - accuracy: 0.9625 - val_loss: 0.0865 - val_accuracy: 0.9881\n",
            "Epoch 56/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.1187 - accuracy: 0.9613 - val_loss: 0.0749 - val_accuracy: 0.9863\n",
            "Epoch 57/100\n",
            "34/34 [==============================] - 3s 103ms/step - loss: 0.1307 - accuracy: 0.9573 - val_loss: 0.0901 - val_accuracy: 0.9872\n",
            "Epoch 58/100\n",
            "34/34 [==============================] - 5s 162ms/step - loss: 0.1190 - accuracy: 0.9623 - val_loss: 0.0776 - val_accuracy: 0.9881\n",
            "Epoch 59/100\n",
            "34/34 [==============================] - 4s 104ms/step - loss: 0.1185 - accuracy: 0.9611 - val_loss: 0.0752 - val_accuracy: 0.9872\n",
            "Epoch 60/100\n",
            "34/34 [==============================] - 4s 105ms/step - loss: 0.1232 - accuracy: 0.9572 - val_loss: 0.0847 - val_accuracy: 0.9872\n",
            "Epoch 61/100\n",
            "34/34 [==============================] - 4s 111ms/step - loss: 0.1047 - accuracy: 0.9714 - val_loss: 0.0702 - val_accuracy: 0.9863\n",
            "Epoch 62/100\n",
            "34/34 [==============================] - 5s 155ms/step - loss: 0.1006 - accuracy: 0.9674 - val_loss: 0.0652 - val_accuracy: 0.9872\n",
            "Epoch 63/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.0923 - accuracy: 0.9744 - val_loss: 0.0642 - val_accuracy: 0.9881\n",
            "Epoch 64/100\n",
            "34/34 [==============================] - 4s 104ms/step - loss: 0.0888 - accuracy: 0.9751 - val_loss: 0.0711 - val_accuracy: 0.9889\n",
            "Epoch 65/100\n",
            "34/34 [==============================] - 4s 123ms/step - loss: 0.1050 - accuracy: 0.9680 - val_loss: 0.0671 - val_accuracy: 0.9906\n",
            "Epoch 66/100\n",
            "34/34 [==============================] - 5s 142ms/step - loss: 0.1009 - accuracy: 0.9693 - val_loss: 0.0610 - val_accuracy: 0.9881\n",
            "Epoch 67/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.0992 - accuracy: 0.9697 - val_loss: 0.0676 - val_accuracy: 0.9889\n",
            "Epoch 68/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.0987 - accuracy: 0.9706 - val_loss: 0.0669 - val_accuracy: 0.9898\n",
            "Epoch 69/100\n",
            "34/34 [==============================] - 5s 137ms/step - loss: 0.0993 - accuracy: 0.9668 - val_loss: 0.0874 - val_accuracy: 0.9863\n",
            "Epoch 70/100\n",
            "34/34 [==============================] - 4s 131ms/step - loss: 0.1115 - accuracy: 0.9634 - val_loss: 0.0664 - val_accuracy: 0.9881\n",
            "Epoch 71/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.0971 - accuracy: 0.9694 - val_loss: 0.0631 - val_accuracy: 0.9889\n",
            "Epoch 72/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1060 - accuracy: 0.9652 - val_loss: 0.0638 - val_accuracy: 0.9906\n",
            "Epoch 73/100\n",
            "34/34 [==============================] - 5s 145ms/step - loss: 0.0970 - accuracy: 0.9711 - val_loss: 0.0587 - val_accuracy: 0.9898\n",
            "Epoch 74/100\n",
            "34/34 [==============================] - 4s 119ms/step - loss: 0.0922 - accuracy: 0.9679 - val_loss: 0.0673 - val_accuracy: 0.9881\n",
            "Epoch 75/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.1175 - accuracy: 0.9584 - val_loss: 0.0717 - val_accuracy: 0.9881\n",
            "Epoch 76/100\n",
            "34/34 [==============================] - 3s 100ms/step - loss: 0.1107 - accuracy: 0.9629 - val_loss: 0.0694 - val_accuracy: 0.9863\n",
            "Epoch 77/100\n",
            "34/34 [==============================] - 5s 158ms/step - loss: 0.1081 - accuracy: 0.9614 - val_loss: 0.0676 - val_accuracy: 0.9898\n",
            "Epoch 78/100\n",
            "34/34 [==============================] - 4s 106ms/step - loss: 0.1150 - accuracy: 0.9596 - val_loss: 0.0958 - val_accuracy: 0.9795\n",
            "Epoch 79/100\n",
            "34/34 [==============================] - 4s 103ms/step - loss: 0.1007 - accuracy: 0.9643 - val_loss: 0.1245 - val_accuracy: 0.9198\n",
            "Epoch 80/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1211 - accuracy: 0.9555 - val_loss: 0.0815 - val_accuracy: 0.9838\n",
            "Epoch 81/100\n",
            "34/34 [==============================] - 5s 161ms/step - loss: 0.1002 - accuracy: 0.9649 - val_loss: 0.0675 - val_accuracy: 0.9881\n",
            "Epoch 82/100\n",
            "34/34 [==============================] - 4s 104ms/step - loss: 0.0912 - accuracy: 0.9711 - val_loss: 0.0704 - val_accuracy: 0.9881\n",
            "Epoch 83/100\n",
            "34/34 [==============================] - 4s 104ms/step - loss: 0.0984 - accuracy: 0.9673 - val_loss: 0.0677 - val_accuracy: 0.9881\n",
            "Epoch 84/100\n",
            "34/34 [==============================] - 4s 119ms/step - loss: 0.1386 - accuracy: 0.9504 - val_loss: 0.0969 - val_accuracy: 0.9855\n",
            "Epoch 85/100\n",
            "34/34 [==============================] - 5s 149ms/step - loss: 0.1416 - accuracy: 0.9447 - val_loss: 0.1077 - val_accuracy: 0.9855\n",
            "Epoch 86/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1551 - accuracy: 0.9372 - val_loss: 0.1106 - val_accuracy: 0.9846\n",
            "Epoch 87/100\n",
            "34/34 [==============================] - 4s 107ms/step - loss: 0.1272 - accuracy: 0.9519 - val_loss: 0.0848 - val_accuracy: 0.9872\n",
            "Epoch 88/100\n",
            "34/34 [==============================] - 4s 131ms/step - loss: 0.1140 - accuracy: 0.9566 - val_loss: 0.0887 - val_accuracy: 0.9855\n",
            "Epoch 89/100\n",
            "34/34 [==============================] - 5s 134ms/step - loss: 0.1096 - accuracy: 0.9590 - val_loss: 0.0742 - val_accuracy: 0.9872\n",
            "Epoch 90/100\n",
            "34/34 [==============================] - 4s 104ms/step - loss: 0.1013 - accuracy: 0.9643 - val_loss: 0.0747 - val_accuracy: 0.9855\n",
            "Epoch 91/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.1192 - accuracy: 0.9516 - val_loss: 0.0782 - val_accuracy: 0.9855\n",
            "Epoch 92/100\n",
            "34/34 [==============================] - 5s 147ms/step - loss: 0.1140 - accuracy: 0.9580 - val_loss: 0.0934 - val_accuracy: 0.9855\n",
            "Epoch 93/100\n",
            "34/34 [==============================] - 4s 118ms/step - loss: 0.0979 - accuracy: 0.9656 - val_loss: 0.0701 - val_accuracy: 0.9881\n",
            "Epoch 94/100\n",
            "34/34 [==============================] - 4s 103ms/step - loss: 0.0884 - accuracy: 0.9670 - val_loss: 0.0696 - val_accuracy: 0.9881\n",
            "Epoch 95/100\n",
            "34/34 [==============================] - 3s 101ms/step - loss: 0.0974 - accuracy: 0.9631 - val_loss: 0.0616 - val_accuracy: 0.9889\n",
            "Epoch 96/100\n",
            "34/34 [==============================] - 5s 158ms/step - loss: 0.0886 - accuracy: 0.9668 - val_loss: 0.0684 - val_accuracy: 0.9872\n",
            "Epoch 97/100\n",
            "34/34 [==============================] - 4s 106ms/step - loss: 0.0929 - accuracy: 0.9682 - val_loss: 0.0636 - val_accuracy: 0.9881\n",
            "Epoch 98/100\n",
            "34/34 [==============================] - 3s 102ms/step - loss: 0.0922 - accuracy: 0.9680 - val_loss: 0.0671 - val_accuracy: 0.9872\n",
            "Epoch 99/100\n",
            "34/34 [==============================] - 4s 105ms/step - loss: 0.0844 - accuracy: 0.9702 - val_loss: 0.0597 - val_accuracy: 0.9906\n",
            "Epoch 100/100\n",
            "34/34 [==============================] - 5s 160ms/step - loss: 0.1010 - accuracy: 0.9638 - val_loss: 0.0690 - val_accuracy: 0.9846\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b7a91b250>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "# model.add(Dense(2**9, activation=\"sigmoid\"))\n",
        "# model.add(Dropout(.4))\n",
        "model.add(Dense(2**11, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**7, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**5, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(18, activation=\"softmax\"))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=300,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS_OzsreGJ8b",
        "outputId": "754debdf-1e60-4114-bde6-90eb2cf5ebf1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 16s 659ms/step - loss: 2.6858 - accuracy: 0.1560 - val_loss: 2.3790 - val_accuracy: 0.3413\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 15s 638ms/step - loss: 2.3791 - accuracy: 0.2618 - val_loss: 2.1050 - val_accuracy: 0.5026\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 15s 641ms/step - loss: 2.1391 - accuracy: 0.3658 - val_loss: 1.8633 - val_accuracy: 0.6493\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 15s 649ms/step - loss: 1.9218 - accuracy: 0.4601 - val_loss: 1.6311 - val_accuracy: 0.7287\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 15s 646ms/step - loss: 1.7242 - accuracy: 0.5305 - val_loss: 1.4435 - val_accuracy: 0.7517\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 14s 635ms/step - loss: 1.5561 - accuracy: 0.5973 - val_loss: 1.2813 - val_accuracy: 0.7585\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 14s 627ms/step - loss: 1.4183 - accuracy: 0.6479 - val_loss: 1.1436 - val_accuracy: 0.7679\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 15s 638ms/step - loss: 1.3035 - accuracy: 0.6784 - val_loss: 1.0249 - val_accuracy: 0.7961\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 21s 934ms/step - loss: 1.1955 - accuracy: 0.7096 - val_loss: 0.9238 - val_accuracy: 0.8063\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 21s 907ms/step - loss: 1.1031 - accuracy: 0.7331 - val_loss: 0.8342 - val_accuracy: 0.8114\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 22s 960ms/step - loss: 1.0269 - accuracy: 0.7463 - val_loss: 0.7636 - val_accuracy: 0.8131\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 16s 708ms/step - loss: 0.9536 - accuracy: 0.7706 - val_loss: 0.6993 - val_accuracy: 0.8242\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 15s 647ms/step - loss: 0.8954 - accuracy: 0.7783 - val_loss: 0.6480 - val_accuracy: 0.8311\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 16s 693ms/step - loss: 0.8433 - accuracy: 0.7812 - val_loss: 0.6010 - val_accuracy: 0.8311\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 16s 698ms/step - loss: 0.7886 - accuracy: 0.7916 - val_loss: 0.5656 - val_accuracy: 0.8302\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 18s 777ms/step - loss: 0.7519 - accuracy: 0.8015 - val_loss: 0.5262 - val_accuracy: 0.8328\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.7220 - accuracy: 0.8099 - val_loss: 0.5002 - val_accuracy: 0.8311\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 17s 720ms/step - loss: 0.6858 - accuracy: 0.8118 - val_loss: 0.4689 - val_accuracy: 0.8319\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 15s 640ms/step - loss: 0.6534 - accuracy: 0.8225 - val_loss: 0.4431 - val_accuracy: 0.8891\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 17s 725ms/step - loss: 0.6303 - accuracy: 0.8312 - val_loss: 0.4302 - val_accuracy: 0.8916\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 16s 698ms/step - loss: 0.6099 - accuracy: 0.8326 - val_loss: 0.4045 - val_accuracy: 0.8754\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 16s 681ms/step - loss: 0.5844 - accuracy: 0.8396 - val_loss: 0.3863 - val_accuracy: 0.9155\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 15s 645ms/step - loss: 0.5721 - accuracy: 0.8384 - val_loss: 0.3635 - val_accuracy: 0.9181\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 15s 649ms/step - loss: 0.5519 - accuracy: 0.8411 - val_loss: 0.3494 - val_accuracy: 0.9164\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 15s 654ms/step - loss: 0.5226 - accuracy: 0.8594 - val_loss: 0.3228 - val_accuracy: 0.9232\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 15s 642ms/step - loss: 0.5153 - accuracy: 0.8524 - val_loss: 0.3067 - val_accuracy: 0.9275\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 15s 668ms/step - loss: 0.4879 - accuracy: 0.8714 - val_loss: 0.3010 - val_accuracy: 0.9181\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 16s 694ms/step - loss: 0.4683 - accuracy: 0.8788 - val_loss: 0.2849 - val_accuracy: 0.9172\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 15s 639ms/step - loss: 0.4617 - accuracy: 0.8745 - val_loss: 0.2680 - val_accuracy: 0.9198\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 15s 643ms/step - loss: 0.4365 - accuracy: 0.8835 - val_loss: 0.2540 - val_accuracy: 0.9232\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 15s 646ms/step - loss: 0.4284 - accuracy: 0.8904 - val_loss: 0.2454 - val_accuracy: 0.9189\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 15s 645ms/step - loss: 0.4148 - accuracy: 0.8928 - val_loss: 0.2285 - val_accuracy: 0.9266\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 15s 649ms/step - loss: 0.4155 - accuracy: 0.8843 - val_loss: 0.2245 - val_accuracy: 0.9898\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 16s 685ms/step - loss: 0.3931 - accuracy: 0.8953 - val_loss: 0.2261 - val_accuracy: 0.9189\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 15s 652ms/step - loss: 0.3918 - accuracy: 0.8901 - val_loss: 0.2101 - val_accuracy: 0.9275\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 19s 857ms/step - loss: 0.3729 - accuracy: 0.8990 - val_loss: 0.1997 - val_accuracy: 0.9369\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 17s 740ms/step - loss: 0.3646 - accuracy: 0.9044 - val_loss: 0.1889 - val_accuracy: 0.9573\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 15s 640ms/step - loss: 0.3500 - accuracy: 0.9129 - val_loss: 0.1938 - val_accuracy: 0.9241\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 15s 642ms/step - loss: 0.3506 - accuracy: 0.9054 - val_loss: 0.1670 - val_accuracy: 0.9829\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 15s 648ms/step - loss: 0.3412 - accuracy: 0.9058 - val_loss: 0.1676 - val_accuracy: 0.9923\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 15s 643ms/step - loss: 0.3604 - accuracy: 0.8930 - val_loss: 0.1797 - val_accuracy: 0.9889\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 14s 630ms/step - loss: 0.3414 - accuracy: 0.9025 - val_loss: 0.1773 - val_accuracy: 0.9863\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 15s 666ms/step - loss: 0.3349 - accuracy: 0.9058 - val_loss: 0.1766 - val_accuracy: 0.9241\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 15s 646ms/step - loss: 0.3199 - accuracy: 0.9141 - val_loss: 0.1881 - val_accuracy: 0.9249\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 15s 645ms/step - loss: 0.3260 - accuracy: 0.9070 - val_loss: 0.1664 - val_accuracy: 0.9232\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 17s 745ms/step - loss: 0.3188 - accuracy: 0.9105 - val_loss: 0.1459 - val_accuracy: 0.9898\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 15s 640ms/step - loss: 0.3052 - accuracy: 0.9114 - val_loss: 0.1868 - val_accuracy: 0.9010\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 15s 653ms/step - loss: 0.3148 - accuracy: 0.9091 - val_loss: 0.1497 - val_accuracy: 0.9889\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 21s 910ms/step - loss: 0.3114 - accuracy: 0.9072 - val_loss: 0.1577 - val_accuracy: 0.9881\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 18s 721ms/step - loss: 0.3003 - accuracy: 0.9106 - val_loss: 0.1671 - val_accuracy: 0.9258\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 16s 706ms/step - loss: 0.3022 - accuracy: 0.9081 - val_loss: 0.1677 - val_accuracy: 0.9206\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 16s 702ms/step - loss: 0.3067 - accuracy: 0.9135 - val_loss: 0.2140 - val_accuracy: 0.8524\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 15s 656ms/step - loss: 0.3263 - accuracy: 0.9014 - val_loss: 0.1519 - val_accuracy: 0.9881\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 17s 726ms/step - loss: 0.2890 - accuracy: 0.9170 - val_loss: 0.1494 - val_accuracy: 0.9241\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 15s 646ms/step - loss: 0.2853 - accuracy: 0.9194 - val_loss: 0.1421 - val_accuracy: 0.9650\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 19s 823ms/step - loss: 0.2796 - accuracy: 0.9203 - val_loss: 0.1706 - val_accuracy: 0.9206\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 17s 757ms/step - loss: 0.2860 - accuracy: 0.9174 - val_loss: 0.1350 - val_accuracy: 0.9881\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 19s 843ms/step - loss: 0.2787 - accuracy: 0.9171 - val_loss: 0.1347 - val_accuracy: 0.9855\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 14s 608ms/step - loss: 0.2701 - accuracy: 0.9230 - val_loss: 0.1712 - val_accuracy: 0.9258\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 14s 605ms/step - loss: 0.2801 - accuracy: 0.9132 - val_loss: 0.1276 - val_accuracy: 0.9915\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 14s 609ms/step - loss: 0.2651 - accuracy: 0.9200 - val_loss: 0.1201 - val_accuracy: 0.9906\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 15s 648ms/step - loss: 0.2745 - accuracy: 0.9198 - val_loss: 0.1292 - val_accuracy: 0.9881\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 14s 614ms/step - loss: 0.2651 - accuracy: 0.9195 - val_loss: 0.1297 - val_accuracy: 0.9872\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 18s 785ms/step - loss: 0.2614 - accuracy: 0.9215 - val_loss: 0.1331 - val_accuracy: 0.9881\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 15s 639ms/step - loss: 0.2528 - accuracy: 0.9275 - val_loss: 0.1485 - val_accuracy: 0.9795\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 14s 617ms/step - loss: 0.2561 - accuracy: 0.9242 - val_loss: 0.1268 - val_accuracy: 0.9906\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 18s 777ms/step - loss: 0.2472 - accuracy: 0.9292 - val_loss: 0.1177 - val_accuracy: 0.9915\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 18s 815ms/step - loss: 0.2477 - accuracy: 0.9242 - val_loss: 0.1240 - val_accuracy: 0.9881\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 15s 638ms/step - loss: 0.2474 - accuracy: 0.9198 - val_loss: 0.1419 - val_accuracy: 0.9241\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 15s 643ms/step - loss: 0.2475 - accuracy: 0.9185 - val_loss: 0.1220 - val_accuracy: 0.9889\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 15s 654ms/step - loss: 0.2423 - accuracy: 0.9248 - val_loss: 0.1224 - val_accuracy: 0.9889\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 15s 639ms/step - loss: 0.2471 - accuracy: 0.9221 - val_loss: 0.1220 - val_accuracy: 0.9684\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 15s 655ms/step - loss: 0.2434 - accuracy: 0.9268 - val_loss: 0.1198 - val_accuracy: 0.9889\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 15s 644ms/step - loss: 0.2281 - accuracy: 0.9308 - val_loss: 0.1166 - val_accuracy: 0.9881\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 18s 806ms/step - loss: 0.2365 - accuracy: 0.9246 - val_loss: 0.1127 - val_accuracy: 0.9889\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 15s 647ms/step - loss: 0.2302 - accuracy: 0.9268 - val_loss: 0.1100 - val_accuracy: 0.9915\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 15s 645ms/step - loss: 0.2321 - accuracy: 0.9295 - val_loss: 0.1125 - val_accuracy: 0.9898\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 15s 643ms/step - loss: 0.2278 - accuracy: 0.9308 - val_loss: 0.1179 - val_accuracy: 0.9906\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 15s 638ms/step - loss: 0.2333 - accuracy: 0.9231 - val_loss: 0.1274 - val_accuracy: 0.9881\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 15s 649ms/step - loss: 0.2396 - accuracy: 0.9219 - val_loss: 0.1199 - val_accuracy: 0.9863\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 15s 645ms/step - loss: 0.2415 - accuracy: 0.9240 - val_loss: 0.1118 - val_accuracy: 0.9881\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 15s 655ms/step - loss: 0.2309 - accuracy: 0.9255 - val_loss: 0.1247 - val_accuracy: 0.9863\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 15s 640ms/step - loss: 0.2284 - accuracy: 0.9272 - val_loss: 0.1660 - val_accuracy: 0.9189\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 15s 639ms/step - loss: 0.2303 - accuracy: 0.9245 - val_loss: 0.1078 - val_accuracy: 0.9906\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 15s 649ms/step - loss: 0.2213 - accuracy: 0.9272 - val_loss: 0.1053 - val_accuracy: 0.9906\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 18s 794ms/step - loss: 0.2270 - accuracy: 0.9237 - val_loss: 0.0995 - val_accuracy: 0.9923\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 16s 686ms/step - loss: 0.2296 - accuracy: 0.9265 - val_loss: 0.1523 - val_accuracy: 0.9164\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 15s 643ms/step - loss: 0.2051 - accuracy: 0.9402 - val_loss: 0.1165 - val_accuracy: 0.9881\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 14s 633ms/step - loss: 0.2080 - accuracy: 0.9325 - val_loss: 0.1101 - val_accuracy: 0.9872\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 15s 644ms/step - loss: 0.2018 - accuracy: 0.9364 - val_loss: 0.1099 - val_accuracy: 0.9889\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 15s 647ms/step - loss: 0.2021 - accuracy: 0.9359 - val_loss: 0.1078 - val_accuracy: 0.9872\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 15s 645ms/step - loss: 0.2007 - accuracy: 0.9367 - val_loss: 0.1072 - val_accuracy: 0.9863\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 15s 665ms/step - loss: 0.2085 - accuracy: 0.9295 - val_loss: 0.1239 - val_accuracy: 0.9829\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 15s 644ms/step - loss: 0.2004 - accuracy: 0.9388 - val_loss: 0.1250 - val_accuracy: 0.9881\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 15s 648ms/step - loss: 0.2165 - accuracy: 0.9266 - val_loss: 0.1101 - val_accuracy: 0.9889\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 15s 638ms/step - loss: 0.2055 - accuracy: 0.9352 - val_loss: 0.1098 - val_accuracy: 0.9881\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 14s 633ms/step - loss: 0.1961 - accuracy: 0.9358 - val_loss: 0.0930 - val_accuracy: 0.9906\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 17s 742ms/step - loss: 0.1983 - accuracy: 0.9364 - val_loss: 0.1018 - val_accuracy: 0.9906\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 15s 642ms/step - loss: 0.1925 - accuracy: 0.9375 - val_loss: 0.1068 - val_accuracy: 0.9872\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 15s 645ms/step - loss: 0.1850 - accuracy: 0.9432 - val_loss: 0.1447 - val_accuracy: 0.9232\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b7fd6f190>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Redo the preprocessing to make the dataset have 900 dimensions\n",
        "\n",
        "# Prepare the data by splitting it into features and target variables\n",
        "X = df.drop([\"Activity\",\"Unnamed: 9001\",\"Unnamed: 9002\"], axis=1)\n",
        "y = df[\"Activity\"]\n",
        "\n",
        "X = X.transpose()\n",
        "\n",
        "# Divide the data into 10 equal parts\n",
        "split_dfs = [X.iloc[i:i+10] for i in range(0, len(X), 10)]\n",
        "\n",
        "# Calculate the mean of each part\n",
        "mean_dfs = [temp_df.mean(axis=0) for temp_df in split_dfs]\n",
        "\n",
        "# Concatenate the means into a single DataFrame\n",
        "result = pd.concat(mean_dfs, axis=1).T\n",
        "\n",
        "result.transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "5d4a1Be-U1cE",
        "outputId": "cea5e41b-2977-48bd-9148-9c37d78f175c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0           1           2           3           4           5    \\\n",
              "0       0.851267    0.694313    0.727255    0.587845    0.867546    0.803658   \n",
              "1       0.553865    0.490857    0.668714    0.365226    0.449540    0.595444   \n",
              "2       0.553980    0.680460    0.524548    0.242357    0.637220    0.546271   \n",
              "3       0.724146    0.372987    0.710398    0.499000    0.627768    0.445024   \n",
              "4       1.241685    0.859600    0.781123    0.803453    0.680757    0.526352   \n",
              "...          ...         ...         ...         ...         ...         ...   \n",
              "9180   78.967200   93.814000  209.089000  134.920300  137.001520  252.824500   \n",
              "9181   95.116500  124.056300  194.590000  155.623300  169.420700  184.293100   \n",
              "9182   92.661900  191.716100  267.030700  322.464000  378.310000  326.276000   \n",
              "9183  106.808800  186.867000  197.496600  264.617800  221.998300  254.182100   \n",
              "9184   81.204900  107.940300  269.550600  149.165900  174.785100  393.077000   \n",
              "\n",
              "             6           7           8           9    ...        890  \\\n",
              "0       0.879808    0.610316    0.742794    1.172012  ...   0.166263   \n",
              "1       0.625260    0.486916    0.822690    0.905982  ...   0.080537   \n",
              "2       0.347287    0.673270    0.594745    1.110250  ...   0.113930   \n",
              "3       0.527460    0.543166    0.818629    0.830290  ...   0.127836   \n",
              "4       0.531985    0.817822    0.568238    0.704233  ...   0.133476   \n",
              "...          ...         ...         ...         ...  ...        ...   \n",
              "9180  164.456400  142.298500  109.630480   82.352700  ...  19.562490   \n",
              "9181  139.294900  120.091700  102.196200   96.668800  ...  18.730500   \n",
              "9182  229.317900  127.245300  143.313300  146.369400  ...  17.749470   \n",
              "9183  246.075000  119.720500  142.935100  141.756000  ...  20.716300   \n",
              "9184  220.084000  156.278500   89.429120  155.513600  ...  18.684110   \n",
              "\n",
              "            891        892        893        894        895        896  \\\n",
              "0      0.186979   0.168182   0.220646   0.111133   0.202796   0.222263   \n",
              "1      0.084053   0.142119   0.120902   0.107982   0.156437   0.102560   \n",
              "2      0.089989   0.074061   0.126068   0.123700   0.176257   0.211640   \n",
              "3      0.122640   0.167529   0.090934   0.106220   0.129944   0.135833   \n",
              "4      0.064636   0.171586   0.064262   0.121755   0.193828   0.123964   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "9180  21.080700  16.882960  20.444620  18.129760  31.226900  25.512720   \n",
              "9181  22.015910  16.929630  25.901800  18.192670  23.372900  31.089380   \n",
              "9182  20.907140  20.745230  18.023620  29.163020  20.810960  29.012100   \n",
              "9183  16.432780  24.455800  18.146240  24.981650  35.540900  24.858940   \n",
              "9184  16.451170  15.112820  19.940010  20.952060  29.068390  24.775720   \n",
              "\n",
              "            897        898        899  \n",
              "0      0.302327   0.368894   2.258764  \n",
              "1      0.147437   0.299035   0.299829  \n",
              "2      0.180813   0.265277   0.425521  \n",
              "3      0.203154   0.299438   0.592899  \n",
              "4      0.235198   0.498932   0.900271  \n",
              "...         ...        ...        ...  \n",
              "9180  33.992720  34.737700  59.015700  \n",
              "9181  31.704400  55.086600  98.186100  \n",
              "9182  41.201100  45.545800  56.041200  \n",
              "9183  30.936210  44.727800  71.661600  \n",
              "9184  34.843000  40.439700  50.312200  \n",
              "\n",
              "[9185 rows x 900 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fa3b7a0-f664-421d-a8b2-5a89ffc6a5dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>890</th>\n",
              "      <th>891</th>\n",
              "      <th>892</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "      <th>898</th>\n",
              "      <th>899</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.851267</td>\n",
              "      <td>0.694313</td>\n",
              "      <td>0.727255</td>\n",
              "      <td>0.587845</td>\n",
              "      <td>0.867546</td>\n",
              "      <td>0.803658</td>\n",
              "      <td>0.879808</td>\n",
              "      <td>0.610316</td>\n",
              "      <td>0.742794</td>\n",
              "      <td>1.172012</td>\n",
              "      <td>...</td>\n",
              "      <td>0.166263</td>\n",
              "      <td>0.186979</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.220646</td>\n",
              "      <td>0.111133</td>\n",
              "      <td>0.202796</td>\n",
              "      <td>0.222263</td>\n",
              "      <td>0.302327</td>\n",
              "      <td>0.368894</td>\n",
              "      <td>2.258764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.553865</td>\n",
              "      <td>0.490857</td>\n",
              "      <td>0.668714</td>\n",
              "      <td>0.365226</td>\n",
              "      <td>0.449540</td>\n",
              "      <td>0.595444</td>\n",
              "      <td>0.625260</td>\n",
              "      <td>0.486916</td>\n",
              "      <td>0.822690</td>\n",
              "      <td>0.905982</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080537</td>\n",
              "      <td>0.084053</td>\n",
              "      <td>0.142119</td>\n",
              "      <td>0.120902</td>\n",
              "      <td>0.107982</td>\n",
              "      <td>0.156437</td>\n",
              "      <td>0.102560</td>\n",
              "      <td>0.147437</td>\n",
              "      <td>0.299035</td>\n",
              "      <td>0.299829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.553980</td>\n",
              "      <td>0.680460</td>\n",
              "      <td>0.524548</td>\n",
              "      <td>0.242357</td>\n",
              "      <td>0.637220</td>\n",
              "      <td>0.546271</td>\n",
              "      <td>0.347287</td>\n",
              "      <td>0.673270</td>\n",
              "      <td>0.594745</td>\n",
              "      <td>1.110250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.113930</td>\n",
              "      <td>0.089989</td>\n",
              "      <td>0.074061</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.123700</td>\n",
              "      <td>0.176257</td>\n",
              "      <td>0.211640</td>\n",
              "      <td>0.180813</td>\n",
              "      <td>0.265277</td>\n",
              "      <td>0.425521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.724146</td>\n",
              "      <td>0.372987</td>\n",
              "      <td>0.710398</td>\n",
              "      <td>0.499000</td>\n",
              "      <td>0.627768</td>\n",
              "      <td>0.445024</td>\n",
              "      <td>0.527460</td>\n",
              "      <td>0.543166</td>\n",
              "      <td>0.818629</td>\n",
              "      <td>0.830290</td>\n",
              "      <td>...</td>\n",
              "      <td>0.127836</td>\n",
              "      <td>0.122640</td>\n",
              "      <td>0.167529</td>\n",
              "      <td>0.090934</td>\n",
              "      <td>0.106220</td>\n",
              "      <td>0.129944</td>\n",
              "      <td>0.135833</td>\n",
              "      <td>0.203154</td>\n",
              "      <td>0.299438</td>\n",
              "      <td>0.592899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.241685</td>\n",
              "      <td>0.859600</td>\n",
              "      <td>0.781123</td>\n",
              "      <td>0.803453</td>\n",
              "      <td>0.680757</td>\n",
              "      <td>0.526352</td>\n",
              "      <td>0.531985</td>\n",
              "      <td>0.817822</td>\n",
              "      <td>0.568238</td>\n",
              "      <td>0.704233</td>\n",
              "      <td>...</td>\n",
              "      <td>0.133476</td>\n",
              "      <td>0.064636</td>\n",
              "      <td>0.171586</td>\n",
              "      <td>0.064262</td>\n",
              "      <td>0.121755</td>\n",
              "      <td>0.193828</td>\n",
              "      <td>0.123964</td>\n",
              "      <td>0.235198</td>\n",
              "      <td>0.498932</td>\n",
              "      <td>0.900271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9180</th>\n",
              "      <td>78.967200</td>\n",
              "      <td>93.814000</td>\n",
              "      <td>209.089000</td>\n",
              "      <td>134.920300</td>\n",
              "      <td>137.001520</td>\n",
              "      <td>252.824500</td>\n",
              "      <td>164.456400</td>\n",
              "      <td>142.298500</td>\n",
              "      <td>109.630480</td>\n",
              "      <td>82.352700</td>\n",
              "      <td>...</td>\n",
              "      <td>19.562490</td>\n",
              "      <td>21.080700</td>\n",
              "      <td>16.882960</td>\n",
              "      <td>20.444620</td>\n",
              "      <td>18.129760</td>\n",
              "      <td>31.226900</td>\n",
              "      <td>25.512720</td>\n",
              "      <td>33.992720</td>\n",
              "      <td>34.737700</td>\n",
              "      <td>59.015700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9181</th>\n",
              "      <td>95.116500</td>\n",
              "      <td>124.056300</td>\n",
              "      <td>194.590000</td>\n",
              "      <td>155.623300</td>\n",
              "      <td>169.420700</td>\n",
              "      <td>184.293100</td>\n",
              "      <td>139.294900</td>\n",
              "      <td>120.091700</td>\n",
              "      <td>102.196200</td>\n",
              "      <td>96.668800</td>\n",
              "      <td>...</td>\n",
              "      <td>18.730500</td>\n",
              "      <td>22.015910</td>\n",
              "      <td>16.929630</td>\n",
              "      <td>25.901800</td>\n",
              "      <td>18.192670</td>\n",
              "      <td>23.372900</td>\n",
              "      <td>31.089380</td>\n",
              "      <td>31.704400</td>\n",
              "      <td>55.086600</td>\n",
              "      <td>98.186100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9182</th>\n",
              "      <td>92.661900</td>\n",
              "      <td>191.716100</td>\n",
              "      <td>267.030700</td>\n",
              "      <td>322.464000</td>\n",
              "      <td>378.310000</td>\n",
              "      <td>326.276000</td>\n",
              "      <td>229.317900</td>\n",
              "      <td>127.245300</td>\n",
              "      <td>143.313300</td>\n",
              "      <td>146.369400</td>\n",
              "      <td>...</td>\n",
              "      <td>17.749470</td>\n",
              "      <td>20.907140</td>\n",
              "      <td>20.745230</td>\n",
              "      <td>18.023620</td>\n",
              "      <td>29.163020</td>\n",
              "      <td>20.810960</td>\n",
              "      <td>29.012100</td>\n",
              "      <td>41.201100</td>\n",
              "      <td>45.545800</td>\n",
              "      <td>56.041200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9183</th>\n",
              "      <td>106.808800</td>\n",
              "      <td>186.867000</td>\n",
              "      <td>197.496600</td>\n",
              "      <td>264.617800</td>\n",
              "      <td>221.998300</td>\n",
              "      <td>254.182100</td>\n",
              "      <td>246.075000</td>\n",
              "      <td>119.720500</td>\n",
              "      <td>142.935100</td>\n",
              "      <td>141.756000</td>\n",
              "      <td>...</td>\n",
              "      <td>20.716300</td>\n",
              "      <td>16.432780</td>\n",
              "      <td>24.455800</td>\n",
              "      <td>18.146240</td>\n",
              "      <td>24.981650</td>\n",
              "      <td>35.540900</td>\n",
              "      <td>24.858940</td>\n",
              "      <td>30.936210</td>\n",
              "      <td>44.727800</td>\n",
              "      <td>71.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9184</th>\n",
              "      <td>81.204900</td>\n",
              "      <td>107.940300</td>\n",
              "      <td>269.550600</td>\n",
              "      <td>149.165900</td>\n",
              "      <td>174.785100</td>\n",
              "      <td>393.077000</td>\n",
              "      <td>220.084000</td>\n",
              "      <td>156.278500</td>\n",
              "      <td>89.429120</td>\n",
              "      <td>155.513600</td>\n",
              "      <td>...</td>\n",
              "      <td>18.684110</td>\n",
              "      <td>16.451170</td>\n",
              "      <td>15.112820</td>\n",
              "      <td>19.940010</td>\n",
              "      <td>20.952060</td>\n",
              "      <td>29.068390</td>\n",
              "      <td>24.775720</td>\n",
              "      <td>34.843000</td>\n",
              "      <td>40.439700</td>\n",
              "      <td>50.312200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9185 rows × 900 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fa3b7a0-f664-421d-a8b2-5a89ffc6a5dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fa3b7a0-f664-421d-a8b2-5a89ffc6a5dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fa3b7a0-f664-421d-a8b2-5a89ffc6a5dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = result.transpose()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_df, y, test_size=0.15, random_state=30)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=31)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.fit_transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "1RxetYgk7nmY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "mB2Ekn0A9bK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c459c6a-b10b-4a5e-80eb-d7e54124d6f2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6635, 900)\n",
            "(6635,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(2**10, activation=\"sigmoid\", input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**7, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**5, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(18, activation=\"softmax\"))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=300,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWikhMfqti0m",
        "outputId": "45e7850a-7525-4b9f-eace-bedba1688531"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 2s 52ms/step - loss: 2.8169 - accuracy: 0.1070 - val_loss: 2.5955 - val_accuracy: 0.1852\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 2.5990 - accuracy: 0.1604 - val_loss: 2.3922 - val_accuracy: 0.2850\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 2.4151 - accuracy: 0.2234 - val_loss: 2.2011 - val_accuracy: 0.4744\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 2.2437 - accuracy: 0.2843 - val_loss: 2.0182 - val_accuracy: 0.5631\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 2.0849 - accuracy: 0.3454 - val_loss: 1.8495 - val_accuracy: 0.6288\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 1.9312 - accuracy: 0.4176 - val_loss: 1.6846 - val_accuracy: 0.6766\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 1.7839 - accuracy: 0.4909 - val_loss: 1.5292 - val_accuracy: 0.6903\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 1.6536 - accuracy: 0.5347 - val_loss: 1.3884 - val_accuracy: 0.6903\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 1.5240 - accuracy: 0.5857 - val_loss: 1.2615 - val_accuracy: 0.7218\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 1.4109 - accuracy: 0.6187 - val_loss: 1.1498 - val_accuracy: 0.7534\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 1.3182 - accuracy: 0.6497 - val_loss: 1.0522 - val_accuracy: 0.7602\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 1s 50ms/step - loss: 1.2313 - accuracy: 0.6714 - val_loss: 0.9710 - val_accuracy: 0.7568\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 1.1563 - accuracy: 0.6981 - val_loss: 0.9015 - val_accuracy: 0.7611\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 1.0946 - accuracy: 0.7023 - val_loss: 0.8395 - val_accuracy: 0.7628\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 1.0289 - accuracy: 0.7225 - val_loss: 0.7838 - val_accuracy: 0.7628\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.9957 - accuracy: 0.7240 - val_loss: 0.7432 - val_accuracy: 0.7611\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.9347 - accuracy: 0.7381 - val_loss: 0.7045 - val_accuracy: 0.8285\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.9009 - accuracy: 0.7530 - val_loss: 0.6638 - val_accuracy: 0.8302\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.8689 - accuracy: 0.7572 - val_loss: 0.6330 - val_accuracy: 0.8294\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.8280 - accuracy: 0.7687 - val_loss: 0.5975 - val_accuracy: 0.8422\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.7942 - accuracy: 0.7711 - val_loss: 0.5711 - val_accuracy: 0.8814\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.7671 - accuracy: 0.7842 - val_loss: 0.5578 - val_accuracy: 0.7654\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.7446 - accuracy: 0.7843 - val_loss: 0.5289 - val_accuracy: 0.8430\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.7150 - accuracy: 0.7986 - val_loss: 0.4978 - val_accuracy: 0.9010\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.6833 - accuracy: 0.8039 - val_loss: 0.4748 - val_accuracy: 0.8916\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.6637 - accuracy: 0.8109 - val_loss: 0.4556 - val_accuracy: 0.8899\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.6438 - accuracy: 0.8145 - val_loss: 0.4439 - val_accuracy: 0.8259\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.6221 - accuracy: 0.8178 - val_loss: 0.4235 - val_accuracy: 0.8942\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.6115 - accuracy: 0.8291 - val_loss: 0.4004 - val_accuracy: 0.9096\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.5923 - accuracy: 0.8310 - val_loss: 0.3902 - val_accuracy: 0.8695\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.5765 - accuracy: 0.8335 - val_loss: 0.3740 - val_accuracy: 0.9164\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 1s 43ms/step - loss: 0.5600 - accuracy: 0.8401 - val_loss: 0.3589 - val_accuracy: 0.9155\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.5431 - accuracy: 0.8523 - val_loss: 0.3448 - val_accuracy: 0.9198\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.5292 - accuracy: 0.8496 - val_loss: 0.3370 - val_accuracy: 0.9147\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.5099 - accuracy: 0.8591 - val_loss: 0.3235 - val_accuracy: 0.9215\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.5008 - accuracy: 0.8647 - val_loss: 0.3102 - val_accuracy: 0.9232\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 2s 69ms/step - loss: 0.4779 - accuracy: 0.8692 - val_loss: 0.3022 - val_accuracy: 0.9283\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.4651 - accuracy: 0.8717 - val_loss: 0.2864 - val_accuracy: 0.9386\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.4668 - accuracy: 0.8731 - val_loss: 0.2774 - val_accuracy: 0.9317\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.4540 - accuracy: 0.8800 - val_loss: 0.2626 - val_accuracy: 0.9548\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.4463 - accuracy: 0.8799 - val_loss: 0.2532 - val_accuracy: 0.9565\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.4333 - accuracy: 0.8846 - val_loss: 0.2476 - val_accuracy: 0.9556\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.4151 - accuracy: 0.8934 - val_loss: 0.2387 - val_accuracy: 0.9556\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.4119 - accuracy: 0.8954 - val_loss: 0.2240 - val_accuracy: 0.9599\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.4006 - accuracy: 0.8998 - val_loss: 0.2166 - val_accuracy: 0.9590\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.3890 - accuracy: 0.9007 - val_loss: 0.2055 - val_accuracy: 0.9616\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 1s 64ms/step - loss: 0.3812 - accuracy: 0.9025 - val_loss: 0.2146 - val_accuracy: 0.9590\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.3664 - accuracy: 0.9096 - val_loss: 0.1867 - val_accuracy: 0.9625\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.3629 - accuracy: 0.9111 - val_loss: 0.1846 - val_accuracy: 0.9625\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 1s 60ms/step - loss: 0.3543 - accuracy: 0.9147 - val_loss: 0.1762 - val_accuracy: 0.9659\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.3481 - accuracy: 0.9144 - val_loss: 0.1695 - val_accuracy: 0.9684\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 1s 43ms/step - loss: 0.3335 - accuracy: 0.9171 - val_loss: 0.1654 - val_accuracy: 0.9642\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 1s 43ms/step - loss: 0.3301 - accuracy: 0.9206 - val_loss: 0.1614 - val_accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.3255 - accuracy: 0.9153 - val_loss: 0.1442 - val_accuracy: 0.9735\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.3107 - accuracy: 0.9252 - val_loss: 0.1402 - val_accuracy: 0.9770\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.3114 - accuracy: 0.9222 - val_loss: 0.1368 - val_accuracy: 0.9829\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.3159 - accuracy: 0.9216 - val_loss: 0.1329 - val_accuracy: 0.9821\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.3006 - accuracy: 0.9302 - val_loss: 0.1342 - val_accuracy: 0.9804\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.2897 - accuracy: 0.9338 - val_loss: 0.1256 - val_accuracy: 0.9881\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 1s 61ms/step - loss: 0.2798 - accuracy: 0.9320 - val_loss: 0.1258 - val_accuracy: 0.9821\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.2886 - accuracy: 0.9328 - val_loss: 0.1145 - val_accuracy: 0.9863\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.2714 - accuracy: 0.9376 - val_loss: 0.1139 - val_accuracy: 0.9881\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 1s 59ms/step - loss: 0.2790 - accuracy: 0.9334 - val_loss: 0.1024 - val_accuracy: 0.9881\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.2645 - accuracy: 0.9391 - val_loss: 0.1057 - val_accuracy: 0.9872\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.2594 - accuracy: 0.9376 - val_loss: 0.0976 - val_accuracy: 0.9855\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.2591 - accuracy: 0.9397 - val_loss: 0.0992 - val_accuracy: 0.9855\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.2457 - accuracy: 0.9444 - val_loss: 0.0879 - val_accuracy: 0.9889\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.2420 - accuracy: 0.9457 - val_loss: 0.0898 - val_accuracy: 0.9898\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.2413 - accuracy: 0.9423 - val_loss: 0.0804 - val_accuracy: 0.9889\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.2330 - accuracy: 0.9465 - val_loss: 0.0933 - val_accuracy: 0.9846\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.2232 - accuracy: 0.9516 - val_loss: 0.0838 - val_accuracy: 0.9872\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.2246 - accuracy: 0.9492 - val_loss: 0.0822 - val_accuracy: 0.9872\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 1s 64ms/step - loss: 0.2157 - accuracy: 0.9507 - val_loss: 0.0815 - val_accuracy: 0.9881\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.2202 - accuracy: 0.9516 - val_loss: 0.0738 - val_accuracy: 0.9889\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 2s 76ms/step - loss: 0.2073 - accuracy: 0.9554 - val_loss: 0.0776 - val_accuracy: 0.9855\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.2066 - accuracy: 0.9536 - val_loss: 0.0689 - val_accuracy: 0.9915\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.2070 - accuracy: 0.9536 - val_loss: 0.0722 - val_accuracy: 0.9881\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 1s 49ms/step - loss: 0.2024 - accuracy: 0.9557 - val_loss: 0.0624 - val_accuracy: 0.9915\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 1s 48ms/step - loss: 0.1960 - accuracy: 0.9563 - val_loss: 0.0639 - val_accuracy: 0.9898\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 1s 47ms/step - loss: 0.2033 - accuracy: 0.9530 - val_loss: 0.0608 - val_accuracy: 0.9923\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.2087 - accuracy: 0.9497 - val_loss: 0.0608 - val_accuracy: 0.9889\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.1943 - accuracy: 0.9586 - val_loss: 0.0565 - val_accuracy: 0.9932\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.1907 - accuracy: 0.9593 - val_loss: 0.0577 - val_accuracy: 0.9915\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.1881 - accuracy: 0.9566 - val_loss: 0.0551 - val_accuracy: 0.9923\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 1s 50ms/step - loss: 0.1865 - accuracy: 0.9573 - val_loss: 0.0629 - val_accuracy: 0.9889\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.1799 - accuracy: 0.9616 - val_loss: 0.0500 - val_accuracy: 0.9932\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.1948 - accuracy: 0.9540 - val_loss: 0.0529 - val_accuracy: 0.9915\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.1890 - accuracy: 0.9527 - val_loss: 0.0476 - val_accuracy: 0.9923\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.1869 - accuracy: 0.9545 - val_loss: 0.0522 - val_accuracy: 0.9923\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.1773 - accuracy: 0.9599 - val_loss: 0.0508 - val_accuracy: 0.9932\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.1734 - accuracy: 0.9607 - val_loss: 0.0448 - val_accuracy: 0.9940\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.1646 - accuracy: 0.9631 - val_loss: 0.0489 - val_accuracy: 0.9906\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.1745 - accuracy: 0.9610 - val_loss: 0.0492 - val_accuracy: 0.9898\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 1s 46ms/step - loss: 0.1705 - accuracy: 0.9604 - val_loss: 0.0416 - val_accuracy: 0.9932\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.1649 - accuracy: 0.9608 - val_loss: 0.0488 - val_accuracy: 0.9915\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.1653 - accuracy: 0.9589 - val_loss: 0.0408 - val_accuracy: 0.9932\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.1636 - accuracy: 0.9616 - val_loss: 0.0453 - val_accuracy: 0.9915\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 1s 63ms/step - loss: 0.1612 - accuracy: 0.9647 - val_loss: 0.0484 - val_accuracy: 0.9898\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.1628 - accuracy: 0.9595 - val_loss: 0.0437 - val_accuracy: 0.9923\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.1486 - accuracy: 0.9687 - val_loss: 0.0402 - val_accuracy: 0.9932\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b7c92cd60>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Redo the preprocessing to make the dataset have 90 dimensions\n",
        "\n",
        "# Prepare the data by splitting it into features and target variables\n",
        "X = df.drop([\"Activity\",\"Unnamed: 9001\",\"Unnamed: 9002\"], axis=1)\n",
        "y = df[\"Activity\"]\n",
        "\n",
        "X = X.transpose()\n",
        "\n",
        "# Divide the data into 10 equal parts\n",
        "split_dfs = [X.iloc[i:i+100] for i in range(0, len(X), 100)]\n",
        "\n",
        "# Calculate the mean of each part\n",
        "mean_dfs = [temp_df.mean(axis=0) for temp_df in split_dfs]\n",
        "\n",
        "# Concatenate the means into a single DataFrame\n",
        "result = pd.concat(mean_dfs, axis=1).T\n",
        "\n",
        "result.transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "3S_Wt8Kzvj6S",
        "outputId": "afffeab3-9a72-43b3-b683-d4189a527158"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0           1          2          3         4         5   \\\n",
              "0       0.793681    1.362187   0.505528   0.382428  0.363849  0.257500   \n",
              "1       0.596449    1.042667   0.446355   0.408206  0.339340  0.254443   \n",
              "2       0.591039    0.945664   0.475361   0.345628  0.370336  0.268479   \n",
              "3       0.609887    1.042276   0.424190   0.356996  0.327986  0.276719   \n",
              "4       0.751525    1.060557   0.444161   0.406685  0.332848  0.269391   \n",
              "...          ...         ...        ...        ...       ...       ...   \n",
              "9180  140.535460   79.708404  26.067734  13.054272  9.657558  6.791733   \n",
              "9181  138.135150   65.278491  19.175211   8.783098  5.169850  3.834974   \n",
              "9182  222.470460  123.014560  26.533188   9.473688  4.994381  4.096854   \n",
              "9183  188.245720   96.121350  19.615885   8.767059  4.630962  2.751710   \n",
              "9184  179.702902  112.086274  35.860440  13.972424  5.605607  4.513116   \n",
              "\n",
              "            6         7         8         9   ...         80         81  \\\n",
              "0     0.255687  0.185089  0.254456  0.255150  ...   0.019208   0.017727   \n",
              "1     0.242139  0.170358  0.237899  0.251250  ...   0.019688   0.014725   \n",
              "2     0.222373  0.226973  0.226832  0.268426  ...   0.020361   0.013918   \n",
              "3     0.232858  0.217640  0.229456  0.279443  ...   0.019640   0.017576   \n",
              "4     0.227035  0.215147  0.230822  0.266333  ...   0.018875   0.015688   \n",
              "...        ...       ...       ...       ...  ...        ...        ...   \n",
              "9180  6.072903  5.769990  6.044316  6.772998  ...  16.991010  16.991020   \n",
              "9181  3.682788  3.618711  3.693385  3.774127  ...  16.957830  16.945990   \n",
              "9182  3.419696  3.459122  3.423397  4.103067  ...  17.201660  17.185190   \n",
              "9183  2.301262  2.180042  2.282370  2.715080  ...  17.251370  17.250820   \n",
              "9184  4.338174  4.717086  4.349052  4.475670  ...  16.974860  16.978380   \n",
              "\n",
              "             82         83         84         85         86         87  \\\n",
              "0      0.018252   0.017890   0.019228   0.023752   0.028972   0.053597   \n",
              "1      0.014303   0.014921   0.018997   0.024073   0.029472   0.042723   \n",
              "2      0.014886   0.013958   0.020278   0.025371   0.028681   0.046179   \n",
              "3      0.014022   0.017929   0.019627   0.025441   0.028903   0.039619   \n",
              "4      0.012223   0.015786   0.018975   0.026976   0.033304   0.045043   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "9180  16.995420  16.994550  16.996930  16.982970  17.008230  16.834960   \n",
              "9181  16.947850  16.944800  16.956390  17.026380  16.750430  17.079080   \n",
              "9182  17.186340  17.184790  17.201350  17.268160  17.120450  17.091040   \n",
              "9183  17.253280  17.250980  17.256940  17.235190  17.298140  17.215120   \n",
              "9184  16.978490  16.978990  16.977660  16.974180  17.011060  16.981380   \n",
              "\n",
              "             88         89  \n",
              "0      0.105601   0.420825  \n",
              "1      0.093598   0.154089  \n",
              "2      0.085961   0.178726  \n",
              "3      0.079141   0.197643  \n",
              "4      0.086572   0.250791  \n",
              "...         ...        ...  \n",
              "9180  17.657064  28.058627  \n",
              "9181  18.351250  34.120989  \n",
              "9182  17.764739  29.919964  \n",
              "9183  17.924539  31.245822  \n",
              "9184  16.980983  27.057918  \n",
              "\n",
              "[9185 rows x 90 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6dc60cb7-9301-43e3-93db-88652baedc5d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.793681</td>\n",
              "      <td>1.362187</td>\n",
              "      <td>0.505528</td>\n",
              "      <td>0.382428</td>\n",
              "      <td>0.363849</td>\n",
              "      <td>0.257500</td>\n",
              "      <td>0.255687</td>\n",
              "      <td>0.185089</td>\n",
              "      <td>0.254456</td>\n",
              "      <td>0.255150</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019208</td>\n",
              "      <td>0.017727</td>\n",
              "      <td>0.018252</td>\n",
              "      <td>0.017890</td>\n",
              "      <td>0.019228</td>\n",
              "      <td>0.023752</td>\n",
              "      <td>0.028972</td>\n",
              "      <td>0.053597</td>\n",
              "      <td>0.105601</td>\n",
              "      <td>0.420825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.596449</td>\n",
              "      <td>1.042667</td>\n",
              "      <td>0.446355</td>\n",
              "      <td>0.408206</td>\n",
              "      <td>0.339340</td>\n",
              "      <td>0.254443</td>\n",
              "      <td>0.242139</td>\n",
              "      <td>0.170358</td>\n",
              "      <td>0.237899</td>\n",
              "      <td>0.251250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019688</td>\n",
              "      <td>0.014725</td>\n",
              "      <td>0.014303</td>\n",
              "      <td>0.014921</td>\n",
              "      <td>0.018997</td>\n",
              "      <td>0.024073</td>\n",
              "      <td>0.029472</td>\n",
              "      <td>0.042723</td>\n",
              "      <td>0.093598</td>\n",
              "      <td>0.154089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.591039</td>\n",
              "      <td>0.945664</td>\n",
              "      <td>0.475361</td>\n",
              "      <td>0.345628</td>\n",
              "      <td>0.370336</td>\n",
              "      <td>0.268479</td>\n",
              "      <td>0.222373</td>\n",
              "      <td>0.226973</td>\n",
              "      <td>0.226832</td>\n",
              "      <td>0.268426</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020361</td>\n",
              "      <td>0.013918</td>\n",
              "      <td>0.014886</td>\n",
              "      <td>0.013958</td>\n",
              "      <td>0.020278</td>\n",
              "      <td>0.025371</td>\n",
              "      <td>0.028681</td>\n",
              "      <td>0.046179</td>\n",
              "      <td>0.085961</td>\n",
              "      <td>0.178726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.609887</td>\n",
              "      <td>1.042276</td>\n",
              "      <td>0.424190</td>\n",
              "      <td>0.356996</td>\n",
              "      <td>0.327986</td>\n",
              "      <td>0.276719</td>\n",
              "      <td>0.232858</td>\n",
              "      <td>0.217640</td>\n",
              "      <td>0.229456</td>\n",
              "      <td>0.279443</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019640</td>\n",
              "      <td>0.017576</td>\n",
              "      <td>0.014022</td>\n",
              "      <td>0.017929</td>\n",
              "      <td>0.019627</td>\n",
              "      <td>0.025441</td>\n",
              "      <td>0.028903</td>\n",
              "      <td>0.039619</td>\n",
              "      <td>0.079141</td>\n",
              "      <td>0.197643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.751525</td>\n",
              "      <td>1.060557</td>\n",
              "      <td>0.444161</td>\n",
              "      <td>0.406685</td>\n",
              "      <td>0.332848</td>\n",
              "      <td>0.269391</td>\n",
              "      <td>0.227035</td>\n",
              "      <td>0.215147</td>\n",
              "      <td>0.230822</td>\n",
              "      <td>0.266333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018875</td>\n",
              "      <td>0.015688</td>\n",
              "      <td>0.012223</td>\n",
              "      <td>0.015786</td>\n",
              "      <td>0.018975</td>\n",
              "      <td>0.026976</td>\n",
              "      <td>0.033304</td>\n",
              "      <td>0.045043</td>\n",
              "      <td>0.086572</td>\n",
              "      <td>0.250791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9180</th>\n",
              "      <td>140.535460</td>\n",
              "      <td>79.708404</td>\n",
              "      <td>26.067734</td>\n",
              "      <td>13.054272</td>\n",
              "      <td>9.657558</td>\n",
              "      <td>6.791733</td>\n",
              "      <td>6.072903</td>\n",
              "      <td>5.769990</td>\n",
              "      <td>6.044316</td>\n",
              "      <td>6.772998</td>\n",
              "      <td>...</td>\n",
              "      <td>16.991010</td>\n",
              "      <td>16.991020</td>\n",
              "      <td>16.995420</td>\n",
              "      <td>16.994550</td>\n",
              "      <td>16.996930</td>\n",
              "      <td>16.982970</td>\n",
              "      <td>17.008230</td>\n",
              "      <td>16.834960</td>\n",
              "      <td>17.657064</td>\n",
              "      <td>28.058627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9181</th>\n",
              "      <td>138.135150</td>\n",
              "      <td>65.278491</td>\n",
              "      <td>19.175211</td>\n",
              "      <td>8.783098</td>\n",
              "      <td>5.169850</td>\n",
              "      <td>3.834974</td>\n",
              "      <td>3.682788</td>\n",
              "      <td>3.618711</td>\n",
              "      <td>3.693385</td>\n",
              "      <td>3.774127</td>\n",
              "      <td>...</td>\n",
              "      <td>16.957830</td>\n",
              "      <td>16.945990</td>\n",
              "      <td>16.947850</td>\n",
              "      <td>16.944800</td>\n",
              "      <td>16.956390</td>\n",
              "      <td>17.026380</td>\n",
              "      <td>16.750430</td>\n",
              "      <td>17.079080</td>\n",
              "      <td>18.351250</td>\n",
              "      <td>34.120989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9182</th>\n",
              "      <td>222.470460</td>\n",
              "      <td>123.014560</td>\n",
              "      <td>26.533188</td>\n",
              "      <td>9.473688</td>\n",
              "      <td>4.994381</td>\n",
              "      <td>4.096854</td>\n",
              "      <td>3.419696</td>\n",
              "      <td>3.459122</td>\n",
              "      <td>3.423397</td>\n",
              "      <td>4.103067</td>\n",
              "      <td>...</td>\n",
              "      <td>17.201660</td>\n",
              "      <td>17.185190</td>\n",
              "      <td>17.186340</td>\n",
              "      <td>17.184790</td>\n",
              "      <td>17.201350</td>\n",
              "      <td>17.268160</td>\n",
              "      <td>17.120450</td>\n",
              "      <td>17.091040</td>\n",
              "      <td>17.764739</td>\n",
              "      <td>29.919964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9183</th>\n",
              "      <td>188.245720</td>\n",
              "      <td>96.121350</td>\n",
              "      <td>19.615885</td>\n",
              "      <td>8.767059</td>\n",
              "      <td>4.630962</td>\n",
              "      <td>2.751710</td>\n",
              "      <td>2.301262</td>\n",
              "      <td>2.180042</td>\n",
              "      <td>2.282370</td>\n",
              "      <td>2.715080</td>\n",
              "      <td>...</td>\n",
              "      <td>17.251370</td>\n",
              "      <td>17.250820</td>\n",
              "      <td>17.253280</td>\n",
              "      <td>17.250980</td>\n",
              "      <td>17.256940</td>\n",
              "      <td>17.235190</td>\n",
              "      <td>17.298140</td>\n",
              "      <td>17.215120</td>\n",
              "      <td>17.924539</td>\n",
              "      <td>31.245822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9184</th>\n",
              "      <td>179.702902</td>\n",
              "      <td>112.086274</td>\n",
              "      <td>35.860440</td>\n",
              "      <td>13.972424</td>\n",
              "      <td>5.605607</td>\n",
              "      <td>4.513116</td>\n",
              "      <td>4.338174</td>\n",
              "      <td>4.717086</td>\n",
              "      <td>4.349052</td>\n",
              "      <td>4.475670</td>\n",
              "      <td>...</td>\n",
              "      <td>16.974860</td>\n",
              "      <td>16.978380</td>\n",
              "      <td>16.978490</td>\n",
              "      <td>16.978990</td>\n",
              "      <td>16.977660</td>\n",
              "      <td>16.974180</td>\n",
              "      <td>17.011060</td>\n",
              "      <td>16.981380</td>\n",
              "      <td>16.980983</td>\n",
              "      <td>27.057918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9185 rows × 90 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dc60cb7-9301-43e3-93db-88652baedc5d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6dc60cb7-9301-43e3-93db-88652baedc5d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6dc60cb7-9301-43e3-93db-88652baedc5d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = result.transpose()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_df, y, test_size=0.15, random_state=30)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=31)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.fit_transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "u-tmHDsYvj3Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNAoXx5jvj0m",
        "outputId": "311e4de5-32ee-4fd7-fa1d-89486882e58a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6635, 90)\n",
            "(6635,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(2**8, activation=\"sigmoid\", input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**6, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(2**4, activation=\"sigmoid\"))\n",
        "model.add(Dropout(.4))\n",
        "model.add(Dense(18, activation=\"softmax\"))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=300,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9GkgpQsvp_A",
        "outputId": "43011a61-fd91-4e63-d2cb-136bb2c0df23"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 1s 18ms/step - loss: 2.9197 - accuracy: 0.0675 - val_loss: 2.8344 - val_accuracy: 0.1015\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.8368 - accuracy: 0.0980 - val_loss: 2.7775 - val_accuracy: 0.1724\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.7798 - accuracy: 0.1186 - val_loss: 2.7141 - val_accuracy: 0.1655\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 2.7287 - accuracy: 0.1287 - val_loss: 2.6493 - val_accuracy: 0.1724\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.6826 - accuracy: 0.1365 - val_loss: 2.5786 - val_accuracy: 0.2577\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.6153 - accuracy: 0.1567 - val_loss: 2.5006 - val_accuracy: 0.2637\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.5483 - accuracy: 0.1604 - val_loss: 2.4234 - val_accuracy: 0.2346\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 2.4789 - accuracy: 0.1815 - val_loss: 2.3469 - val_accuracy: 0.2432\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.4157 - accuracy: 0.1955 - val_loss: 2.2718 - val_accuracy: 0.2619\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.3479 - accuracy: 0.2062 - val_loss: 2.2012 - val_accuracy: 0.3259\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.2997 - accuracy: 0.2121 - val_loss: 2.1378 - val_accuracy: 0.3720\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 2.2449 - accuracy: 0.2223 - val_loss: 2.0810 - val_accuracy: 0.3754\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.1914 - accuracy: 0.2323 - val_loss: 2.0273 - val_accuracy: 0.3669\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.1406 - accuracy: 0.2482 - val_loss: 1.9783 - val_accuracy: 0.4070\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.1047 - accuracy: 0.2493 - val_loss: 1.9353 - val_accuracy: 0.3473\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 2.0671 - accuracy: 0.2648 - val_loss: 1.8946 - val_accuracy: 0.3584\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 2.0279 - accuracy: 0.2644 - val_loss: 1.8579 - val_accuracy: 0.3584\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.9930 - accuracy: 0.2659 - val_loss: 1.8242 - val_accuracy: 0.3532\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.9575 - accuracy: 0.2698 - val_loss: 1.7945 - val_accuracy: 0.3507\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.9375 - accuracy: 0.2850 - val_loss: 1.7666 - val_accuracy: 0.3584\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.9138 - accuracy: 0.2791 - val_loss: 1.7420 - val_accuracy: 0.3447\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.8818 - accuracy: 0.2945 - val_loss: 1.7184 - val_accuracy: 0.3456\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.8682 - accuracy: 0.2977 - val_loss: 1.6993 - val_accuracy: 0.3430\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.8500 - accuracy: 0.2974 - val_loss: 1.6777 - val_accuracy: 0.3345\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.8270 - accuracy: 0.3017 - val_loss: 1.6568 - val_accuracy: 0.3490\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.8054 - accuracy: 0.3103 - val_loss: 1.6395 - val_accuracy: 0.3507\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.7927 - accuracy: 0.3182 - val_loss: 1.6232 - val_accuracy: 0.3345\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.7722 - accuracy: 0.3114 - val_loss: 1.6079 - val_accuracy: 0.3575\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.7440 - accuracy: 0.3266 - val_loss: 1.5920 - val_accuracy: 0.3541\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.7461 - accuracy: 0.3237 - val_loss: 1.5797 - val_accuracy: 0.3703\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.7305 - accuracy: 0.3176 - val_loss: 1.5648 - val_accuracy: 0.3695\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.7184 - accuracy: 0.3311 - val_loss: 1.5499 - val_accuracy: 0.3780\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.7140 - accuracy: 0.3228 - val_loss: 1.5391 - val_accuracy: 0.3567\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.6903 - accuracy: 0.3356 - val_loss: 1.5251 - val_accuracy: 0.3541\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.6792 - accuracy: 0.3338 - val_loss: 1.5155 - val_accuracy: 0.3823\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.6705 - accuracy: 0.3280 - val_loss: 1.5026 - val_accuracy: 0.3626\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.6617 - accuracy: 0.3349 - val_loss: 1.4864 - val_accuracy: 0.3831\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.6460 - accuracy: 0.3444 - val_loss: 1.4788 - val_accuracy: 0.3840\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.6457 - accuracy: 0.3369 - val_loss: 1.4645 - val_accuracy: 0.3857\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.6254 - accuracy: 0.3465 - val_loss: 1.4532 - val_accuracy: 0.4676\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.6260 - accuracy: 0.3510 - val_loss: 1.4461 - val_accuracy: 0.4428\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.6022 - accuracy: 0.3534 - val_loss: 1.4349 - val_accuracy: 0.4462\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5931 - accuracy: 0.3608 - val_loss: 1.4224 - val_accuracy: 0.4420\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.5985 - accuracy: 0.3468 - val_loss: 1.4159 - val_accuracy: 0.4573\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5843 - accuracy: 0.3498 - val_loss: 1.4079 - val_accuracy: 0.4684\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.5735 - accuracy: 0.3557 - val_loss: 1.3961 - val_accuracy: 0.4923\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5705 - accuracy: 0.3573 - val_loss: 1.3905 - val_accuracy: 0.4881\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5584 - accuracy: 0.3590 - val_loss: 1.3800 - val_accuracy: 0.4633\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.5502 - accuracy: 0.3549 - val_loss: 1.3703 - val_accuracy: 0.4795\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.5407 - accuracy: 0.3581 - val_loss: 1.3642 - val_accuracy: 0.4497\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5350 - accuracy: 0.3607 - val_loss: 1.3553 - val_accuracy: 0.4821\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5385 - accuracy: 0.3592 - val_loss: 1.3440 - val_accuracy: 0.4812\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5202 - accuracy: 0.3595 - val_loss: 1.3396 - val_accuracy: 0.4872\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5228 - accuracy: 0.3563 - val_loss: 1.3286 - val_accuracy: 0.4812\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5094 - accuracy: 0.3673 - val_loss: 1.3173 - val_accuracy: 0.4718\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.4966 - accuracy: 0.3659 - val_loss: 1.3074 - val_accuracy: 0.4744\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.4972 - accuracy: 0.3671 - val_loss: 1.2994 - val_accuracy: 0.5401\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4913 - accuracy: 0.3687 - val_loss: 1.2871 - val_accuracy: 0.5392\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4764 - accuracy: 0.3848 - val_loss: 1.2760 - val_accuracy: 0.5077\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4762 - accuracy: 0.3806 - val_loss: 1.2634 - val_accuracy: 0.5435\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.4558 - accuracy: 0.3866 - val_loss: 1.2567 - val_accuracy: 0.5580\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4503 - accuracy: 0.3840 - val_loss: 1.2430 - val_accuracy: 0.5683\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4439 - accuracy: 0.4029 - val_loss: 1.2303 - val_accuracy: 0.5495\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.4409 - accuracy: 0.3952 - val_loss: 1.2248 - val_accuracy: 0.5316\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4403 - accuracy: 0.3946 - val_loss: 1.2142 - val_accuracy: 0.6220\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4152 - accuracy: 0.4008 - val_loss: 1.1981 - val_accuracy: 0.5751\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.4184 - accuracy: 0.4023 - val_loss: 1.1845 - val_accuracy: 0.5503\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.4127 - accuracy: 0.3938 - val_loss: 1.1748 - val_accuracy: 0.4889\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3898 - accuracy: 0.4045 - val_loss: 1.1648 - val_accuracy: 0.5000\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.3836 - accuracy: 0.4000 - val_loss: 1.1521 - val_accuracy: 0.5068\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3844 - accuracy: 0.4045 - val_loss: 1.1471 - val_accuracy: 0.5094\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.3735 - accuracy: 0.3934 - val_loss: 1.1347 - val_accuracy: 0.5017\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3720 - accuracy: 0.3946 - val_loss: 1.1268 - val_accuracy: 0.5094\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3570 - accuracy: 0.3953 - val_loss: 1.1195 - val_accuracy: 0.5009\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3608 - accuracy: 0.3885 - val_loss: 1.1128 - val_accuracy: 0.5043\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3543 - accuracy: 0.4020 - val_loss: 1.1080 - val_accuracy: 0.4915\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.3444 - accuracy: 0.4030 - val_loss: 1.1034 - val_accuracy: 0.5077\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3353 - accuracy: 0.4125 - val_loss: 1.0960 - val_accuracy: 0.5094\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3318 - accuracy: 0.3980 - val_loss: 1.0929 - val_accuracy: 0.5017\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.3204 - accuracy: 0.4206 - val_loss: 1.0874 - val_accuracy: 0.5102\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3347 - accuracy: 0.4050 - val_loss: 1.0852 - val_accuracy: 0.4829\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3156 - accuracy: 0.4167 - val_loss: 1.0816 - val_accuracy: 0.4804\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3202 - accuracy: 0.4068 - val_loss: 1.0759 - val_accuracy: 0.4915\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3134 - accuracy: 0.4093 - val_loss: 1.0719 - val_accuracy: 0.5026\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3035 - accuracy: 0.4166 - val_loss: 1.0694 - val_accuracy: 0.5051\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.3061 - accuracy: 0.4039 - val_loss: 1.0669 - val_accuracy: 0.5119\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.3044 - accuracy: 0.4087 - val_loss: 1.0638 - val_accuracy: 0.5051\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2978 - accuracy: 0.4146 - val_loss: 1.0597 - val_accuracy: 0.5017\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.3001 - accuracy: 0.4157 - val_loss: 1.0556 - val_accuracy: 0.5094\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2923 - accuracy: 0.4146 - val_loss: 1.0510 - val_accuracy: 0.5111\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.2862 - accuracy: 0.4145 - val_loss: 1.0510 - val_accuracy: 0.5102\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2848 - accuracy: 0.4145 - val_loss: 1.0495 - val_accuracy: 0.5043\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2938 - accuracy: 0.4199 - val_loss: 1.0474 - val_accuracy: 0.5077\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2933 - accuracy: 0.4086 - val_loss: 1.0434 - val_accuracy: 0.5102\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2817 - accuracy: 0.4115 - val_loss: 1.0391 - val_accuracy: 0.5085\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2870 - accuracy: 0.4137 - val_loss: 1.0421 - val_accuracy: 0.5077\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2709 - accuracy: 0.4310 - val_loss: 1.0321 - val_accuracy: 0.5085\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2661 - accuracy: 0.4241 - val_loss: 1.0384 - val_accuracy: 0.5085\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2710 - accuracy: 0.4197 - val_loss: 1.0281 - val_accuracy: 0.5094\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.2625 - accuracy: 0.4253 - val_loss: 1.0316 - val_accuracy: 0.5085\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b82c7f4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This model is not training very well. This is likely due to the loss of information\n",
        "# in the 'clumsy' way of chunking down the dimensions. I'll go back to 900 dimensions.\n",
        "\n",
        "# Prepare the data by splitting it into features and target variables\n",
        "X = df.drop([\"Activity\",\"Unnamed: 9001\",\"Unnamed: 9002\"], axis=1)\n",
        "y = df[\"Activity\"]\n",
        "\n",
        "X = X.transpose()\n",
        "\n",
        "# Divide the data into 10 equal parts\n",
        "split_dfs = [X.iloc[i:i+10] for i in range(0, len(X), 10)]\n",
        "\n",
        "# Calculate the mean of each part\n",
        "mean_dfs = [temp_df.mean(axis=0) for temp_df in split_dfs]\n",
        "\n",
        "# Concatenate the means into a single DataFrame\n",
        "result = pd.concat(mean_dfs, axis=1).T\n",
        "\n",
        "result.transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "2x-K9vqw3djZ",
        "outputId": "2f9c105e-057d-491a-84c9-7862171ab8a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0           1           2           3           4           5    \\\n",
              "0       0.851267    0.694313    0.727255    0.587845    0.867546    0.803658   \n",
              "1       0.553865    0.490857    0.668714    0.365226    0.449540    0.595444   \n",
              "2       0.553980    0.680460    0.524548    0.242357    0.637220    0.546271   \n",
              "3       0.724146    0.372987    0.710398    0.499000    0.627768    0.445024   \n",
              "4       1.241685    0.859600    0.781123    0.803453    0.680757    0.526352   \n",
              "...          ...         ...         ...         ...         ...         ...   \n",
              "9180   78.967200   93.814000  209.089000  134.920300  137.001520  252.824500   \n",
              "9181   95.116500  124.056300  194.590000  155.623300  169.420700  184.293100   \n",
              "9182   92.661900  191.716100  267.030700  322.464000  378.310000  326.276000   \n",
              "9183  106.808800  186.867000  197.496600  264.617800  221.998300  254.182100   \n",
              "9184   81.204900  107.940300  269.550600  149.165900  174.785100  393.077000   \n",
              "\n",
              "             6           7           8           9    ...        890  \\\n",
              "0       0.879808    0.610316    0.742794    1.172012  ...   0.166263   \n",
              "1       0.625260    0.486916    0.822690    0.905982  ...   0.080537   \n",
              "2       0.347287    0.673270    0.594745    1.110250  ...   0.113930   \n",
              "3       0.527460    0.543166    0.818629    0.830290  ...   0.127836   \n",
              "4       0.531985    0.817822    0.568238    0.704233  ...   0.133476   \n",
              "...          ...         ...         ...         ...  ...        ...   \n",
              "9180  164.456400  142.298500  109.630480   82.352700  ...  19.562490   \n",
              "9181  139.294900  120.091700  102.196200   96.668800  ...  18.730500   \n",
              "9182  229.317900  127.245300  143.313300  146.369400  ...  17.749470   \n",
              "9183  246.075000  119.720500  142.935100  141.756000  ...  20.716300   \n",
              "9184  220.084000  156.278500   89.429120  155.513600  ...  18.684110   \n",
              "\n",
              "            891        892        893        894        895        896  \\\n",
              "0      0.186979   0.168182   0.220646   0.111133   0.202796   0.222263   \n",
              "1      0.084053   0.142119   0.120902   0.107982   0.156437   0.102560   \n",
              "2      0.089989   0.074061   0.126068   0.123700   0.176257   0.211640   \n",
              "3      0.122640   0.167529   0.090934   0.106220   0.129944   0.135833   \n",
              "4      0.064636   0.171586   0.064262   0.121755   0.193828   0.123964   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "9180  21.080700  16.882960  20.444620  18.129760  31.226900  25.512720   \n",
              "9181  22.015910  16.929630  25.901800  18.192670  23.372900  31.089380   \n",
              "9182  20.907140  20.745230  18.023620  29.163020  20.810960  29.012100   \n",
              "9183  16.432780  24.455800  18.146240  24.981650  35.540900  24.858940   \n",
              "9184  16.451170  15.112820  19.940010  20.952060  29.068390  24.775720   \n",
              "\n",
              "            897        898        899  \n",
              "0      0.302327   0.368894   2.258764  \n",
              "1      0.147437   0.299035   0.299829  \n",
              "2      0.180813   0.265277   0.425521  \n",
              "3      0.203154   0.299438   0.592899  \n",
              "4      0.235198   0.498932   0.900271  \n",
              "...         ...        ...        ...  \n",
              "9180  33.992720  34.737700  59.015700  \n",
              "9181  31.704400  55.086600  98.186100  \n",
              "9182  41.201100  45.545800  56.041200  \n",
              "9183  30.936210  44.727800  71.661600  \n",
              "9184  34.843000  40.439700  50.312200  \n",
              "\n",
              "[9185 rows x 900 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f07dd076-a4a1-4341-818e-dfcd3428540f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>890</th>\n",
              "      <th>891</th>\n",
              "      <th>892</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "      <th>898</th>\n",
              "      <th>899</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.851267</td>\n",
              "      <td>0.694313</td>\n",
              "      <td>0.727255</td>\n",
              "      <td>0.587845</td>\n",
              "      <td>0.867546</td>\n",
              "      <td>0.803658</td>\n",
              "      <td>0.879808</td>\n",
              "      <td>0.610316</td>\n",
              "      <td>0.742794</td>\n",
              "      <td>1.172012</td>\n",
              "      <td>...</td>\n",
              "      <td>0.166263</td>\n",
              "      <td>0.186979</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.220646</td>\n",
              "      <td>0.111133</td>\n",
              "      <td>0.202796</td>\n",
              "      <td>0.222263</td>\n",
              "      <td>0.302327</td>\n",
              "      <td>0.368894</td>\n",
              "      <td>2.258764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.553865</td>\n",
              "      <td>0.490857</td>\n",
              "      <td>0.668714</td>\n",
              "      <td>0.365226</td>\n",
              "      <td>0.449540</td>\n",
              "      <td>0.595444</td>\n",
              "      <td>0.625260</td>\n",
              "      <td>0.486916</td>\n",
              "      <td>0.822690</td>\n",
              "      <td>0.905982</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080537</td>\n",
              "      <td>0.084053</td>\n",
              "      <td>0.142119</td>\n",
              "      <td>0.120902</td>\n",
              "      <td>0.107982</td>\n",
              "      <td>0.156437</td>\n",
              "      <td>0.102560</td>\n",
              "      <td>0.147437</td>\n",
              "      <td>0.299035</td>\n",
              "      <td>0.299829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.553980</td>\n",
              "      <td>0.680460</td>\n",
              "      <td>0.524548</td>\n",
              "      <td>0.242357</td>\n",
              "      <td>0.637220</td>\n",
              "      <td>0.546271</td>\n",
              "      <td>0.347287</td>\n",
              "      <td>0.673270</td>\n",
              "      <td>0.594745</td>\n",
              "      <td>1.110250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.113930</td>\n",
              "      <td>0.089989</td>\n",
              "      <td>0.074061</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.123700</td>\n",
              "      <td>0.176257</td>\n",
              "      <td>0.211640</td>\n",
              "      <td>0.180813</td>\n",
              "      <td>0.265277</td>\n",
              "      <td>0.425521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.724146</td>\n",
              "      <td>0.372987</td>\n",
              "      <td>0.710398</td>\n",
              "      <td>0.499000</td>\n",
              "      <td>0.627768</td>\n",
              "      <td>0.445024</td>\n",
              "      <td>0.527460</td>\n",
              "      <td>0.543166</td>\n",
              "      <td>0.818629</td>\n",
              "      <td>0.830290</td>\n",
              "      <td>...</td>\n",
              "      <td>0.127836</td>\n",
              "      <td>0.122640</td>\n",
              "      <td>0.167529</td>\n",
              "      <td>0.090934</td>\n",
              "      <td>0.106220</td>\n",
              "      <td>0.129944</td>\n",
              "      <td>0.135833</td>\n",
              "      <td>0.203154</td>\n",
              "      <td>0.299438</td>\n",
              "      <td>0.592899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.241685</td>\n",
              "      <td>0.859600</td>\n",
              "      <td>0.781123</td>\n",
              "      <td>0.803453</td>\n",
              "      <td>0.680757</td>\n",
              "      <td>0.526352</td>\n",
              "      <td>0.531985</td>\n",
              "      <td>0.817822</td>\n",
              "      <td>0.568238</td>\n",
              "      <td>0.704233</td>\n",
              "      <td>...</td>\n",
              "      <td>0.133476</td>\n",
              "      <td>0.064636</td>\n",
              "      <td>0.171586</td>\n",
              "      <td>0.064262</td>\n",
              "      <td>0.121755</td>\n",
              "      <td>0.193828</td>\n",
              "      <td>0.123964</td>\n",
              "      <td>0.235198</td>\n",
              "      <td>0.498932</td>\n",
              "      <td>0.900271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9180</th>\n",
              "      <td>78.967200</td>\n",
              "      <td>93.814000</td>\n",
              "      <td>209.089000</td>\n",
              "      <td>134.920300</td>\n",
              "      <td>137.001520</td>\n",
              "      <td>252.824500</td>\n",
              "      <td>164.456400</td>\n",
              "      <td>142.298500</td>\n",
              "      <td>109.630480</td>\n",
              "      <td>82.352700</td>\n",
              "      <td>...</td>\n",
              "      <td>19.562490</td>\n",
              "      <td>21.080700</td>\n",
              "      <td>16.882960</td>\n",
              "      <td>20.444620</td>\n",
              "      <td>18.129760</td>\n",
              "      <td>31.226900</td>\n",
              "      <td>25.512720</td>\n",
              "      <td>33.992720</td>\n",
              "      <td>34.737700</td>\n",
              "      <td>59.015700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9181</th>\n",
              "      <td>95.116500</td>\n",
              "      <td>124.056300</td>\n",
              "      <td>194.590000</td>\n",
              "      <td>155.623300</td>\n",
              "      <td>169.420700</td>\n",
              "      <td>184.293100</td>\n",
              "      <td>139.294900</td>\n",
              "      <td>120.091700</td>\n",
              "      <td>102.196200</td>\n",
              "      <td>96.668800</td>\n",
              "      <td>...</td>\n",
              "      <td>18.730500</td>\n",
              "      <td>22.015910</td>\n",
              "      <td>16.929630</td>\n",
              "      <td>25.901800</td>\n",
              "      <td>18.192670</td>\n",
              "      <td>23.372900</td>\n",
              "      <td>31.089380</td>\n",
              "      <td>31.704400</td>\n",
              "      <td>55.086600</td>\n",
              "      <td>98.186100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9182</th>\n",
              "      <td>92.661900</td>\n",
              "      <td>191.716100</td>\n",
              "      <td>267.030700</td>\n",
              "      <td>322.464000</td>\n",
              "      <td>378.310000</td>\n",
              "      <td>326.276000</td>\n",
              "      <td>229.317900</td>\n",
              "      <td>127.245300</td>\n",
              "      <td>143.313300</td>\n",
              "      <td>146.369400</td>\n",
              "      <td>...</td>\n",
              "      <td>17.749470</td>\n",
              "      <td>20.907140</td>\n",
              "      <td>20.745230</td>\n",
              "      <td>18.023620</td>\n",
              "      <td>29.163020</td>\n",
              "      <td>20.810960</td>\n",
              "      <td>29.012100</td>\n",
              "      <td>41.201100</td>\n",
              "      <td>45.545800</td>\n",
              "      <td>56.041200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9183</th>\n",
              "      <td>106.808800</td>\n",
              "      <td>186.867000</td>\n",
              "      <td>197.496600</td>\n",
              "      <td>264.617800</td>\n",
              "      <td>221.998300</td>\n",
              "      <td>254.182100</td>\n",
              "      <td>246.075000</td>\n",
              "      <td>119.720500</td>\n",
              "      <td>142.935100</td>\n",
              "      <td>141.756000</td>\n",
              "      <td>...</td>\n",
              "      <td>20.716300</td>\n",
              "      <td>16.432780</td>\n",
              "      <td>24.455800</td>\n",
              "      <td>18.146240</td>\n",
              "      <td>24.981650</td>\n",
              "      <td>35.540900</td>\n",
              "      <td>24.858940</td>\n",
              "      <td>30.936210</td>\n",
              "      <td>44.727800</td>\n",
              "      <td>71.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9184</th>\n",
              "      <td>81.204900</td>\n",
              "      <td>107.940300</td>\n",
              "      <td>269.550600</td>\n",
              "      <td>149.165900</td>\n",
              "      <td>174.785100</td>\n",
              "      <td>393.077000</td>\n",
              "      <td>220.084000</td>\n",
              "      <td>156.278500</td>\n",
              "      <td>89.429120</td>\n",
              "      <td>155.513600</td>\n",
              "      <td>...</td>\n",
              "      <td>18.684110</td>\n",
              "      <td>16.451170</td>\n",
              "      <td>15.112820</td>\n",
              "      <td>19.940010</td>\n",
              "      <td>20.952060</td>\n",
              "      <td>29.068390</td>\n",
              "      <td>24.775720</td>\n",
              "      <td>34.843000</td>\n",
              "      <td>40.439700</td>\n",
              "      <td>50.312200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9185 rows × 900 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f07dd076-a4a1-4341-818e-dfcd3428540f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f07dd076-a4a1-4341-818e-dfcd3428540f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f07dd076-a4a1-4341-818e-dfcd3428540f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = result.transpose()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_df, y, test_size=0.15, random_state=30)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=31)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.fit_transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Ua1QPRMH3cd1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcgXYNEU3cZv",
        "outputId": "b95cfdb8-4fae-4011-d15b-971702083ba7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6635, 900)\n",
            "(6635,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do a manual hp grid search\n",
        "from keras.optimizers.optimizer_v2.optimizer_v2 import learning_rate_schedule\n",
        "layers_list = [3,4,5]\n",
        "batch_sizes = [100,300]\n",
        "lrs = [.01,.001]\n",
        "dropouts = [.2,.35,.5]\n",
        "\n",
        "models_to_fit = (len(layers_list) * len(batch_sizes) * len(lrs)\n",
        "                * len(dropouts))\n",
        "\n",
        "hps = []\n",
        "val_losses = []\n",
        "count = 0\n",
        "\n",
        "for layers in layers_list:\n",
        "  for dropout in dropouts:\n",
        "    for batch_size in batch_sizes:\n",
        "      for lr in lrs:\n",
        "        model = Sequential()\n",
        "        model.add(Dense(2**10, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
        "        model.add(Dropout(dropout))\n",
        "        if layers == 3:\n",
        "          model.add(Dense(2**8, activation='sigmoid'))\n",
        "          model.add(Dropout(dropout))\n",
        "        elif layers == 4:\n",
        "          model.add(Dense(2**9, activation='sigmoid'))\n",
        "          model.add(Dropout(dropout))\n",
        "          model.add(Dense(2**7, activation='sigmoid'))\n",
        "          model.add(Dropout(dropout))\n",
        "        elif layers == 5:\n",
        "          model.add(Dense(2**9, activation='sigmoid'))\n",
        "          model.add(Dropout(dropout))\n",
        "          model.add(Dense(2**8, activation='sigmoid'))\n",
        "          model.add(Dropout(dropout))\n",
        "          model.add(Dense(2**7, activation='sigmoid'))\n",
        "          model.add(Dropout(dropout))\n",
        "        model.add(Dense(64, activation='sigmoid'))\n",
        "        model.add(Dense(18, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=keras.optimizers.Adam(lr=lr),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=20,\n",
        "                            verbose=0,\n",
        "                            validation_data=(X_val, y_val))\n",
        "        count += 1\n",
        "        print(\"loss:\", history.history['val_loss'][-1])\n",
        "        print(\"dropout:\", dropout, \"batch_size:\", batch_size, \"lr:\", lr)\n",
        "        print(\"layers:\", layers)\n",
        "        print(\"models fitted:\", count, \"/\", models_to_fit)\n",
        "        print(\"----------------------------------------------\")\n",
        "\n",
        "        hps.append({\"dropout\" : dropout, \n",
        "                    \"batch_size\" : batch_size,\n",
        "                    \"lr\" : lr,\n",
        "                    \"layers\" : layers})\n",
        "        val_losses.append(history.history['val_loss'][-1])\n",
        "\n",
        "print(\"The best_hps are:\", hps[np.argmin(val_losses)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_yfbJACJvu4",
        "outputId": "026a524f-1457-4930-82ce-bb15839b58e6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.26103293895721436\n",
            "dropout: 0.2 batch_size: 100 lr: 0.01\n",
            "layers: 3\n",
            "models fitted: 1 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.03791316971182823\n",
            "dropout: 0.2 batch_size: 100 lr: 0.001\n",
            "layers: 3\n",
            "models fitted: 2 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.24841120839118958\n",
            "dropout: 0.2 batch_size: 300 lr: 0.01\n",
            "layers: 3\n",
            "models fitted: 3 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.14608033001422882\n",
            "dropout: 0.2 batch_size: 300 lr: 0.001\n",
            "layers: 3\n",
            "models fitted: 4 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.2539302110671997\n",
            "dropout: 0.35 batch_size: 100 lr: 0.01\n",
            "layers: 3\n",
            "models fitted: 5 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.08382485061883926\n",
            "dropout: 0.35 batch_size: 100 lr: 0.001\n",
            "layers: 3\n",
            "models fitted: 6 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.3399825692176819\n",
            "dropout: 0.35 batch_size: 300 lr: 0.01\n",
            "layers: 3\n",
            "models fitted: 7 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.20280735194683075\n",
            "dropout: 0.35 batch_size: 300 lr: 0.001\n",
            "layers: 3\n",
            "models fitted: 8 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.3120648264884949\n",
            "dropout: 0.5 batch_size: 100 lr: 0.01\n",
            "layers: 3\n",
            "models fitted: 9 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.058775629848241806\n",
            "dropout: 0.5 batch_size: 100 lr: 0.001\n",
            "layers: 3\n",
            "models fitted: 10 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.2596501410007477\n",
            "dropout: 0.5 batch_size: 300 lr: 0.01\n",
            "layers: 3\n",
            "models fitted: 11 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.19917245209217072\n",
            "dropout: 0.5 batch_size: 300 lr: 0.001\n",
            "layers: 3\n",
            "models fitted: 12 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.3897686004638672\n",
            "dropout: 0.2 batch_size: 100 lr: 0.01\n",
            "layers: 4\n",
            "models fitted: 13 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.07624475657939911\n",
            "dropout: 0.2 batch_size: 100 lr: 0.001\n",
            "layers: 4\n",
            "models fitted: 14 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.6523101329803467\n",
            "dropout: 0.2 batch_size: 300 lr: 0.01\n",
            "layers: 4\n",
            "models fitted: 15 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.3121505677700043\n",
            "dropout: 0.2 batch_size: 300 lr: 0.001\n",
            "layers: 4\n",
            "models fitted: 16 / 36\n",
            "----------------------------------------------\n",
            "loss: 1.2837005853652954\n",
            "dropout: 0.35 batch_size: 100 lr: 0.01\n",
            "layers: 4\n",
            "models fitted: 17 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.1023607850074768\n",
            "dropout: 0.35 batch_size: 100 lr: 0.001\n",
            "layers: 4\n",
            "models fitted: 18 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.9032318592071533\n",
            "dropout: 0.35 batch_size: 300 lr: 0.01\n",
            "layers: 4\n",
            "models fitted: 19 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.33630216121673584\n",
            "dropout: 0.35 batch_size: 300 lr: 0.001\n",
            "layers: 4\n",
            "models fitted: 20 / 36\n",
            "----------------------------------------------\n",
            "loss: 1.1590481996536255\n",
            "dropout: 0.5 batch_size: 100 lr: 0.01\n",
            "layers: 4\n",
            "models fitted: 21 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.1423509269952774\n",
            "dropout: 0.5 batch_size: 100 lr: 0.001\n",
            "layers: 4\n",
            "models fitted: 22 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.9847211241722107\n",
            "dropout: 0.5 batch_size: 300 lr: 0.01\n",
            "layers: 4\n",
            "models fitted: 23 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.5032688975334167\n",
            "dropout: 0.5 batch_size: 300 lr: 0.001\n",
            "layers: 4\n",
            "models fitted: 24 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.7979835271835327\n",
            "dropout: 0.2 batch_size: 100 lr: 0.01\n",
            "layers: 5\n",
            "models fitted: 25 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.20669496059417725\n",
            "dropout: 0.2 batch_size: 100 lr: 0.001\n",
            "layers: 5\n",
            "models fitted: 26 / 36\n",
            "----------------------------------------------\n",
            "loss: 1.1244052648544312\n",
            "dropout: 0.2 batch_size: 300 lr: 0.01\n",
            "layers: 5\n",
            "models fitted: 27 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.5939089059829712\n",
            "dropout: 0.2 batch_size: 300 lr: 0.001\n",
            "layers: 5\n",
            "models fitted: 28 / 36\n",
            "----------------------------------------------\n",
            "loss: 1.2299718856811523\n",
            "dropout: 0.35 batch_size: 100 lr: 0.01\n",
            "layers: 5\n",
            "models fitted: 29 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.36505794525146484\n",
            "dropout: 0.35 batch_size: 100 lr: 0.001\n",
            "layers: 5\n",
            "models fitted: 30 / 36\n",
            "----------------------------------------------\n",
            "loss: 1.2977231740951538\n",
            "dropout: 0.35 batch_size: 300 lr: 0.01\n",
            "layers: 5\n",
            "models fitted: 31 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.7326738834381104\n",
            "dropout: 0.35 batch_size: 300 lr: 0.001\n",
            "layers: 5\n",
            "models fitted: 32 / 36\n",
            "----------------------------------------------\n",
            "loss: 1.2469745874404907\n",
            "dropout: 0.5 batch_size: 100 lr: 0.01\n",
            "layers: 5\n",
            "models fitted: 33 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.8541356921195984\n",
            "dropout: 0.5 batch_size: 100 lr: 0.001\n",
            "layers: 5\n",
            "models fitted: 34 / 36\n",
            "----------------------------------------------\n",
            "loss: 1.1958709955215454\n",
            "dropout: 0.5 batch_size: 300 lr: 0.01\n",
            "layers: 5\n",
            "models fitted: 35 / 36\n",
            "----------------------------------------------\n",
            "loss: 0.9274913668632507\n",
            "dropout: 0.5 batch_size: 300 lr: 0.001\n",
            "layers: 5\n",
            "models fitted: 36 / 36\n",
            "----------------------------------------------\n",
            "The best_hps are: {'dropout': 0.2, 'batch_size': 100, 'lr': 0.001, 'layers': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best_hps are:\", hps[np.argmin(val_losses)])\n",
        "print(\"loss: \", np.min(val_losses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECHRVy5hJvhZ",
        "outputId": "f5d19c54-2098-4c0e-f0cf-da551c749fb3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best_hps are: {'dropout': 0.2, 'batch_size': 100, 'lr': 0.001, 'layers': 3}\n",
            "loss:  0.03791316971182823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = hps[np.argmin(val_losses)]['dropout']\n",
        "batch_size = hps[np.argmin(val_losses)]['batch_size']\n",
        "lr = hps[np.argmin(val_losses)]['lr']\n",
        "layers = hps[np.argmin(val_losses)]['layers']"
      ],
      "metadata": {
        "id": "ndYbLhK_Yn6U"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After looking back through the results of the grid search, the loss was only\n",
        "# slightly higher on the model with Dropout = 0.5. This model will likely\n",
        "# generalize better, so I will train each model again, and see which does better.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(2**10, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(.5))\n",
        "if layers == 3:\n",
        "  model.add(Dense(2**8, activation='sigmoid'))\n",
        "  model.add(Dropout(.5))\n",
        "elif layers == 4:\n",
        "  model.add(Dense(2**9, activation='sigmoid'))\n",
        "  model.add(Dropout(.5))\n",
        "  model.add(Dense(2**7, activation='sigmoid'))\n",
        "  model.add(Dropout(dropout))\n",
        "elif layers == 5:\n",
        "  model.add(Dense(2**9, activation='sigmoid'))\n",
        "  model.add(Dropout(.5))\n",
        "  model.add(Dense(2**8, activation='sigmoid'))\n",
        "  model.add(Dropout(.5))\n",
        "  model.add(Dense(2**7, activation='sigmoid'))\n",
        "  model.add(Dropout(.5))\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "model.add(Dense(18, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(lr=lr),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "cnwUq1WU-B2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229677fe-714a-488f-cb62-34fb7946e67f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 2s 28ms/step - loss: 2.4607 - accuracy: 0.2249 - val_loss: 1.9757 - val_accuracy: 0.4206\n",
            "Epoch 2/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 1.6965 - accuracy: 0.5387 - val_loss: 1.2929 - val_accuracy: 0.6374\n",
            "Epoch 3/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 1.1483 - accuracy: 0.6937 - val_loss: 0.8808 - val_accuracy: 0.6775\n",
            "Epoch 4/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.8327 - accuracy: 0.7730 - val_loss: 0.6625 - val_accuracy: 0.8404\n",
            "Epoch 5/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.6459 - accuracy: 0.8408 - val_loss: 0.5194 - val_accuracy: 0.8618\n",
            "Epoch 6/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.5286 - accuracy: 0.8619 - val_loss: 0.4255 - val_accuracy: 0.9027\n",
            "Epoch 7/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.4288 - accuracy: 0.8975 - val_loss: 0.3382 - val_accuracy: 0.9522\n",
            "Epoch 8/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.3647 - accuracy: 0.9115 - val_loss: 0.2783 - val_accuracy: 0.9590\n",
            "Epoch 9/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.3132 - accuracy: 0.9308 - val_loss: 0.2478 - val_accuracy: 0.9275\n",
            "Epoch 10/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.2762 - accuracy: 0.9358 - val_loss: 0.1983 - val_accuracy: 0.9744\n",
            "Epoch 11/100\n",
            "67/67 [==============================] - 2s 36ms/step - loss: 0.2322 - accuracy: 0.9545 - val_loss: 0.1642 - val_accuracy: 0.9795\n",
            "Epoch 12/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.1987 - accuracy: 0.9644 - val_loss: 0.1411 - val_accuracy: 0.9804\n",
            "Epoch 13/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.1852 - accuracy: 0.9634 - val_loss: 0.1253 - val_accuracy: 0.9778\n",
            "Epoch 14/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.1571 - accuracy: 0.9712 - val_loss: 0.0982 - val_accuracy: 0.9889\n",
            "Epoch 15/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.1461 - accuracy: 0.9726 - val_loss: 0.0844 - val_accuracy: 0.9881\n",
            "Epoch 16/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.1243 - accuracy: 0.9781 - val_loss: 0.0763 - val_accuracy: 0.9898\n",
            "Epoch 17/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.1139 - accuracy: 0.9795 - val_loss: 0.0739 - val_accuracy: 0.9855\n",
            "Epoch 18/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.1027 - accuracy: 0.9804 - val_loss: 0.0717 - val_accuracy: 0.9872\n",
            "Epoch 19/100\n",
            "67/67 [==============================] - 2s 35ms/step - loss: 0.0955 - accuracy: 0.9800 - val_loss: 0.0618 - val_accuracy: 0.9881\n",
            "Epoch 20/100\n",
            "67/67 [==============================] - 3s 39ms/step - loss: 0.0837 - accuracy: 0.9852 - val_loss: 0.0577 - val_accuracy: 0.9898\n",
            "Epoch 21/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0823 - accuracy: 0.9834 - val_loss: 0.0610 - val_accuracy: 0.9821\n",
            "Epoch 22/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0771 - accuracy: 0.9852 - val_loss: 0.0521 - val_accuracy: 0.9898\n",
            "Epoch 23/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0880 - accuracy: 0.9751 - val_loss: 0.0619 - val_accuracy: 0.9940\n",
            "Epoch 24/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0708 - accuracy: 0.9836 - val_loss: 0.0394 - val_accuracy: 0.9898\n",
            "Epoch 25/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0683 - accuracy: 0.9843 - val_loss: 0.0406 - val_accuracy: 0.9889\n",
            "Epoch 26/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0707 - accuracy: 0.9833 - val_loss: 0.0340 - val_accuracy: 0.9923\n",
            "Epoch 27/100\n",
            "67/67 [==============================] - 2s 35ms/step - loss: 0.0551 - accuracy: 0.9878 - val_loss: 0.0332 - val_accuracy: 0.9915\n",
            "Epoch 28/100\n",
            "67/67 [==============================] - 3s 40ms/step - loss: 0.0521 - accuracy: 0.9870 - val_loss: 0.0251 - val_accuracy: 0.9940\n",
            "Epoch 29/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0499 - accuracy: 0.9879 - val_loss: 0.0237 - val_accuracy: 0.9940\n",
            "Epoch 30/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0605 - accuracy: 0.9819 - val_loss: 0.0358 - val_accuracy: 0.9915\n",
            "Epoch 31/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0492 - accuracy: 0.9866 - val_loss: 0.0270 - val_accuracy: 0.9915\n",
            "Epoch 32/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0495 - accuracy: 0.9864 - val_loss: 0.0433 - val_accuracy: 0.9855\n",
            "Epoch 33/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0397 - accuracy: 0.9910 - val_loss: 0.0371 - val_accuracy: 0.9898\n",
            "Epoch 34/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0384 - accuracy: 0.9920 - val_loss: 0.0294 - val_accuracy: 0.9906\n",
            "Epoch 35/100\n",
            "67/67 [==============================] - 2s 35ms/step - loss: 0.0436 - accuracy: 0.9893 - val_loss: 0.0246 - val_accuracy: 0.9949\n",
            "Epoch 36/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0444 - accuracy: 0.9887 - val_loss: 0.0250 - val_accuracy: 0.9949\n",
            "Epoch 37/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0437 - accuracy: 0.9888 - val_loss: 0.0218 - val_accuracy: 0.9940\n",
            "Epoch 38/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0368 - accuracy: 0.9902 - val_loss: 0.0202 - val_accuracy: 0.9949\n",
            "Epoch 39/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0373 - accuracy: 0.9914 - val_loss: 0.0180 - val_accuracy: 0.9957\n",
            "Epoch 40/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0417 - accuracy: 0.9887 - val_loss: 0.0229 - val_accuracy: 0.9940\n",
            "Epoch 41/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0352 - accuracy: 0.9902 - val_loss: 0.0210 - val_accuracy: 0.9949\n",
            "Epoch 42/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.0210 - val_accuracy: 0.9932\n",
            "Epoch 43/100\n",
            "67/67 [==============================] - 2s 33ms/step - loss: 0.0281 - accuracy: 0.9926 - val_loss: 0.0130 - val_accuracy: 0.9949\n",
            "Epoch 44/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0280 - accuracy: 0.9919 - val_loss: 0.0159 - val_accuracy: 0.9949\n",
            "Epoch 45/100\n",
            "67/67 [==============================] - 2s 29ms/step - loss: 0.0333 - accuracy: 0.9901 - val_loss: 0.0139 - val_accuracy: 0.9957\n",
            "Epoch 46/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0361 - accuracy: 0.9899 - val_loss: 0.0205 - val_accuracy: 0.9932\n",
            "Epoch 47/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0290 - accuracy: 0.9922 - val_loss: 0.0186 - val_accuracy: 0.9940\n",
            "Epoch 48/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0287 - accuracy: 0.9923 - val_loss: 0.0222 - val_accuracy: 0.9940\n",
            "Epoch 49/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0258 - accuracy: 0.9938 - val_loss: 0.0148 - val_accuracy: 0.9949\n",
            "Epoch 50/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0280 - accuracy: 0.9919 - val_loss: 0.0237 - val_accuracy: 0.9940\n",
            "Epoch 51/100\n",
            "67/67 [==============================] - 2s 30ms/step - loss: 0.0661 - accuracy: 0.9771 - val_loss: 0.0156 - val_accuracy: 0.9949\n",
            "Epoch 52/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0266 - accuracy: 0.9928 - val_loss: 0.0208 - val_accuracy: 0.9932\n",
            "Epoch 53/100\n",
            "67/67 [==============================] - 2s 31ms/step - loss: 0.0207 - accuracy: 0.9952 - val_loss: 0.0127 - val_accuracy: 0.9949\n",
            "Epoch 54/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0206 - accuracy: 0.9941 - val_loss: 0.0151 - val_accuracy: 0.9932\n",
            "Epoch 55/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0188 - accuracy: 0.9950 - val_loss: 0.0139 - val_accuracy: 0.9949\n",
            "Epoch 56/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0185 - accuracy: 0.9953 - val_loss: 0.0132 - val_accuracy: 0.9957\n",
            "Epoch 57/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.0397 - val_accuracy: 0.9855\n",
            "Epoch 58/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0213 - accuracy: 0.9940 - val_loss: 0.0151 - val_accuracy: 0.9957\n",
            "Epoch 59/100\n",
            "67/67 [==============================] - 2s 29ms/step - loss: 0.0293 - accuracy: 0.9910 - val_loss: 0.0097 - val_accuracy: 0.9991\n",
            "Epoch 60/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0344 - accuracy: 0.9896 - val_loss: 0.0109 - val_accuracy: 0.9983\n",
            "Epoch 61/100\n",
            "67/67 [==============================] - 2s 33ms/step - loss: 0.0194 - accuracy: 0.9953 - val_loss: 0.0086 - val_accuracy: 0.9974\n",
            "Epoch 62/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0393 - accuracy: 0.9855 - val_loss: 0.0168 - val_accuracy: 0.9940\n",
            "Epoch 63/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0332 - accuracy: 0.9905 - val_loss: 0.0153 - val_accuracy: 0.9949\n",
            "Epoch 64/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0212 - accuracy: 0.9940 - val_loss: 0.0149 - val_accuracy: 0.9932\n",
            "Epoch 65/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0178 - accuracy: 0.9956 - val_loss: 0.0123 - val_accuracy: 0.9966\n",
            "Epoch 66/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0236 - accuracy: 0.9935 - val_loss: 0.0130 - val_accuracy: 0.9957\n",
            "Epoch 67/100\n",
            "67/67 [==============================] - 2s 29ms/step - loss: 0.0155 - accuracy: 0.9964 - val_loss: 0.0108 - val_accuracy: 0.9949\n",
            "Epoch 68/100\n",
            "67/67 [==============================] - 3s 39ms/step - loss: 0.0185 - accuracy: 0.9950 - val_loss: 0.0130 - val_accuracy: 0.9966\n",
            "Epoch 69/100\n",
            "67/67 [==============================] - 2s 33ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.0125 - val_accuracy: 0.9957\n",
            "Epoch 70/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0209 - accuracy: 0.9937 - val_loss: 0.0176 - val_accuracy: 0.9923\n",
            "Epoch 71/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0251 - accuracy: 0.9931 - val_loss: 0.0141 - val_accuracy: 0.9949\n",
            "Epoch 72/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0236 - accuracy: 0.9926 - val_loss: 0.0075 - val_accuracy: 0.9966\n",
            "Epoch 73/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0171 - accuracy: 0.9949 - val_loss: 0.0059 - val_accuracy: 0.9983\n",
            "Epoch 74/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0297 - accuracy: 0.9902 - val_loss: 0.1167 - val_accuracy: 0.9582\n",
            "Epoch 75/100\n",
            "67/67 [==============================] - 2s 30ms/step - loss: 0.0350 - accuracy: 0.9888 - val_loss: 0.0135 - val_accuracy: 0.9949\n",
            "Epoch 76/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0155 - accuracy: 0.9959 - val_loss: 0.0110 - val_accuracy: 0.9957\n",
            "Epoch 77/100\n",
            "67/67 [==============================] - 2s 34ms/step - loss: 0.0215 - accuracy: 0.9934 - val_loss: 0.0093 - val_accuracy: 0.9983\n",
            "Epoch 78/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0250 - accuracy: 0.9925 - val_loss: 0.0144 - val_accuracy: 0.9949\n",
            "Epoch 79/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 0.0228 - val_accuracy: 0.9906\n",
            "Epoch 80/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0277 - accuracy: 0.9899 - val_loss: 0.0157 - val_accuracy: 0.9932\n",
            "Epoch 81/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.0191 - val_accuracy: 0.9923\n",
            "Epoch 82/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.0104 - val_accuracy: 0.9940\n",
            "Epoch 83/100\n",
            "67/67 [==============================] - 2s 30ms/step - loss: 0.0210 - accuracy: 0.9946 - val_loss: 0.0127 - val_accuracy: 0.9949\n",
            "Epoch 84/100\n",
            "67/67 [==============================] - 3s 39ms/step - loss: 0.0599 - accuracy: 0.9778 - val_loss: 0.0099 - val_accuracy: 0.9966\n",
            "Epoch 85/100\n",
            "67/67 [==============================] - 2s 33ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.0112 - val_accuracy: 0.9957\n",
            "Epoch 86/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0130 - accuracy: 0.9967 - val_loss: 0.0175 - val_accuracy: 0.9949\n",
            "Epoch 87/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 0.0152 - val_accuracy: 0.9949\n",
            "Epoch 88/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0189 - accuracy: 0.9934 - val_loss: 0.0104 - val_accuracy: 0.9957\n",
            "Epoch 89/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0249 - accuracy: 0.9922 - val_loss: 0.0099 - val_accuracy: 0.9940\n",
            "Epoch 90/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0282 - accuracy: 0.9901 - val_loss: 0.0164 - val_accuracy: 0.9966\n",
            "Epoch 91/100\n",
            "67/67 [==============================] - 2s 31ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.0248 - val_accuracy: 0.9906\n",
            "Epoch 92/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.0126 - val_accuracy: 0.9957\n",
            "Epoch 93/100\n",
            "67/67 [==============================] - 2s 32ms/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.0144 - val_accuracy: 0.9957\n",
            "Epoch 94/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0249 - accuracy: 0.9931 - val_loss: 0.0117 - val_accuracy: 0.9966\n",
            "Epoch 95/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0217 - accuracy: 0.9943 - val_loss: 0.0068 - val_accuracy: 0.9983\n",
            "Epoch 96/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.0126 - val_accuracy: 0.9957\n",
            "Epoch 97/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0050 - val_accuracy: 0.9974\n",
            "Epoch 98/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.0139 - val_accuracy: 0.9949\n",
            "Epoch 99/100\n",
            "67/67 [==============================] - 2s 28ms/step - loss: 0.0180 - accuracy: 0.9946 - val_loss: 0.0059 - val_accuracy: 0.9983\n",
            "Epoch 100/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0186 - accuracy: 0.9943 - val_loss: 0.0151 - val_accuracy: 0.9940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(2**10, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(dropout))\n",
        "if layers == 3:\n",
        "  model.add(Dense(2**8, activation='sigmoid'))\n",
        "  model.add(Dropout(dropout))\n",
        "elif layers == 4:\n",
        "  model.add(Dense(2**9, activation='sigmoid'))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(2**7, activation='sigmoid'))\n",
        "  model.add(Dropout(dropout))\n",
        "elif layers == 5:\n",
        "  model.add(Dense(2**9, activation='sigmoid'))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(2**8, activation='sigmoid'))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(2**7, activation='sigmoid'))\n",
        "  model.add(Dropout(dropout))\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "model.add(Dense(18, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(lr=lr),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_CF0NYnB6r0",
        "outputId": "4cde4cba-80b1-4ef1-e9a9-bd5a54196fca"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "67/67 [==============================] - 3s 27ms/step - loss: 2.2986 - accuracy: 0.3075 - val_loss: 1.7558 - val_accuracy: 0.5631\n",
            "Epoch 2/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 1.3569 - accuracy: 0.6972 - val_loss: 0.9866 - val_accuracy: 0.7253\n",
            "Epoch 3/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.8287 - accuracy: 0.7833 - val_loss: 0.6705 - val_accuracy: 0.8413\n",
            "Epoch 4/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.5900 - accuracy: 0.8532 - val_loss: 0.5034 - val_accuracy: 0.8729\n",
            "Epoch 5/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.4474 - accuracy: 0.8939 - val_loss: 0.3870 - val_accuracy: 0.8882\n",
            "Epoch 6/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.3384 - accuracy: 0.9415 - val_loss: 0.2945 - val_accuracy: 0.9625\n",
            "Epoch 7/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.2566 - accuracy: 0.9662 - val_loss: 0.2340 - val_accuracy: 0.9744\n",
            "Epoch 8/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.2045 - accuracy: 0.9736 - val_loss: 0.1746 - val_accuracy: 0.9812\n",
            "Epoch 9/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.1606 - accuracy: 0.9827 - val_loss: 0.1306 - val_accuracy: 0.9855\n",
            "Epoch 10/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.1273 - accuracy: 0.9869 - val_loss: 0.1058 - val_accuracy: 0.9872\n",
            "Epoch 11/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.1100 - accuracy: 0.9872 - val_loss: 0.0842 - val_accuracy: 0.9923\n",
            "Epoch 12/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0878 - accuracy: 0.9917 - val_loss: 0.0799 - val_accuracy: 0.9872\n",
            "Epoch 13/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0713 - accuracy: 0.9929 - val_loss: 0.0614 - val_accuracy: 0.9915\n",
            "Epoch 14/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0736 - accuracy: 0.9882 - val_loss: 0.0593 - val_accuracy: 0.9881\n",
            "Epoch 15/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0565 - accuracy: 0.9938 - val_loss: 0.0611 - val_accuracy: 0.9872\n",
            "Epoch 16/100\n",
            "67/67 [==============================] - 2s 30ms/step - loss: 0.0588 - accuracy: 0.9898 - val_loss: 0.0442 - val_accuracy: 0.9940\n",
            "Epoch 17/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0531 - accuracy: 0.9914 - val_loss: 0.2085 - val_accuracy: 0.8754\n",
            "Epoch 18/100\n",
            "67/67 [==============================] - 2s 28ms/step - loss: 0.1068 - accuracy: 0.9727 - val_loss: 0.0565 - val_accuracy: 0.9863\n",
            "Epoch 19/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0400 - accuracy: 0.9952 - val_loss: 0.0329 - val_accuracy: 0.9915\n",
            "Epoch 20/100\n",
            "67/67 [==============================] - 2s 35ms/step - loss: 0.0333 - accuracy: 0.9964 - val_loss: 0.0413 - val_accuracy: 0.9906\n",
            "Epoch 21/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0340 - accuracy: 0.9955 - val_loss: 0.0310 - val_accuracy: 0.9923\n",
            "Epoch 22/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0293 - accuracy: 0.9964 - val_loss: 0.0331 - val_accuracy: 0.9923\n",
            "Epoch 23/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0391 - accuracy: 0.9908 - val_loss: 0.0336 - val_accuracy: 0.9940\n",
            "Epoch 24/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0238 - accuracy: 0.9970 - val_loss: 0.0227 - val_accuracy: 0.9906\n",
            "Epoch 25/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0188 - accuracy: 0.9983 - val_loss: 0.0220 - val_accuracy: 0.9974\n",
            "Epoch 26/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0225 - accuracy: 0.9967 - val_loss: 0.0191 - val_accuracy: 0.9957\n",
            "Epoch 27/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0197 - accuracy: 0.9976 - val_loss: 0.0193 - val_accuracy: 0.9949\n",
            "Epoch 28/100\n",
            "67/67 [==============================] - 2s 35ms/step - loss: 0.0257 - accuracy: 0.9943 - val_loss: 0.0167 - val_accuracy: 0.9957\n",
            "Epoch 29/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0153 - accuracy: 0.9977 - val_loss: 0.0207 - val_accuracy: 0.9966\n",
            "Epoch 30/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0148 - accuracy: 0.9983 - val_loss: 0.0153 - val_accuracy: 0.9966\n",
            "Epoch 31/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0151 - accuracy: 0.9974 - val_loss: 0.0145 - val_accuracy: 0.9957\n",
            "Epoch 32/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0158 - accuracy: 0.9973 - val_loss: 0.0213 - val_accuracy: 0.9949\n",
            "Epoch 33/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0176 - accuracy: 0.9964 - val_loss: 0.0494 - val_accuracy: 0.9838\n",
            "Epoch 34/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0138 - accuracy: 0.9974 - val_loss: 0.0119 - val_accuracy: 0.9966\n",
            "Epoch 35/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0149 - accuracy: 0.9971 - val_loss: 0.0170 - val_accuracy: 0.9923\n",
            "Epoch 36/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.0151 - val_accuracy: 0.9957\n",
            "Epoch 37/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0121 - accuracy: 0.9974 - val_loss: 0.0237 - val_accuracy: 0.9906\n",
            "Epoch 38/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0084 - accuracy: 0.9989 - val_loss: 0.0178 - val_accuracy: 0.9957\n",
            "Epoch 39/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0072 - accuracy: 0.9994 - val_loss: 0.0165 - val_accuracy: 0.9940\n",
            "Epoch 40/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.0129 - val_accuracy: 0.9940\n",
            "Epoch 41/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 0.0261 - val_accuracy: 0.9923\n",
            "Epoch 42/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0062 - accuracy: 0.9995 - val_loss: 0.0071 - val_accuracy: 0.9991\n",
            "Epoch 43/100\n",
            "67/67 [==============================] - 3s 39ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.0139 - val_accuracy: 0.9949\n",
            "Epoch 44/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0141 - accuracy: 0.9970 - val_loss: 0.0217 - val_accuracy: 0.9940\n",
            "Epoch 45/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 0.0139 - val_accuracy: 0.9949\n",
            "Epoch 46/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0124 - accuracy: 0.9973 - val_loss: 0.0186 - val_accuracy: 0.9966\n",
            "Epoch 47/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.0193 - val_accuracy: 0.9923\n",
            "Epoch 48/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0061 - accuracy: 0.9992 - val_loss: 0.0127 - val_accuracy: 0.9957\n",
            "Epoch 49/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 0.0070 - val_accuracy: 0.9983\n",
            "Epoch 50/100\n",
            "67/67 [==============================] - 2s 27ms/step - loss: 0.0085 - accuracy: 0.9977 - val_loss: 0.0483 - val_accuracy: 0.9863\n",
            "Epoch 51/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0115 - accuracy: 0.9974 - val_loss: 0.0134 - val_accuracy: 0.9966\n",
            "Epoch 52/100\n",
            "67/67 [==============================] - 2s 35ms/step - loss: 0.0099 - accuracy: 0.9982 - val_loss: 0.0161 - val_accuracy: 0.9949\n",
            "Epoch 53/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 0.0123 - val_accuracy: 0.9957\n",
            "Epoch 54/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.0061 - val_accuracy: 0.9983\n",
            "Epoch 55/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 0.0122 - val_accuracy: 0.9957\n",
            "Epoch 56/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0115 - val_accuracy: 0.9966\n",
            "Epoch 57/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.0192 - val_accuracy: 0.9940\n",
            "Epoch 58/100\n",
            "67/67 [==============================] - 2s 27ms/step - loss: 0.0092 - accuracy: 0.9980 - val_loss: 0.0140 - val_accuracy: 0.9957\n",
            "Epoch 59/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0079 - val_accuracy: 0.9974\n",
            "Epoch 60/100\n",
            "67/67 [==============================] - 2s 36ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.0197 - val_accuracy: 0.9940\n",
            "Epoch 61/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.0107 - val_accuracy: 0.9966\n",
            "Epoch 62/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0085 - val_accuracy: 0.9957\n",
            "Epoch 63/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0138 - val_accuracy: 0.9940\n",
            "Epoch 64/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 0.9957\n",
            "Epoch 65/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.0045 - val_accuracy: 0.9974\n",
            "Epoch 66/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.0152 - val_accuracy: 0.9940\n",
            "Epoch 67/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0110 - accuracy: 0.9971 - val_loss: 0.0104 - val_accuracy: 0.9957\n",
            "Epoch 68/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.0094 - val_accuracy: 0.9957\n",
            "Epoch 69/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0102 - val_accuracy: 0.9949\n",
            "Epoch 70/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.0073 - val_accuracy: 0.9966\n",
            "Epoch 71/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 0.9940\n",
            "Epoch 72/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.0189 - val_accuracy: 0.9932\n",
            "Epoch 73/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0116 - val_accuracy: 0.9940\n",
            "Epoch 74/100\n",
            "67/67 [==============================] - 2s 27ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0153 - val_accuracy: 0.9957\n",
            "Epoch 75/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0608 - accuracy: 0.9819 - val_loss: 0.0271 - val_accuracy: 0.9906\n",
            "Epoch 76/100\n",
            "67/67 [==============================] - 2s 37ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.0180 - val_accuracy: 0.9949\n",
            "Epoch 77/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.0095 - val_accuracy: 0.9957\n",
            "Epoch 78/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0053 - val_accuracy: 0.9974\n",
            "Epoch 79/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0074 - val_accuracy: 0.9974\n",
            "Epoch 80/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 0.9966\n",
            "Epoch 81/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0120 - val_accuracy: 0.9957\n",
            "Epoch 82/100\n",
            "67/67 [==============================] - 2s 29ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0103 - val_accuracy: 0.9966\n",
            "Epoch 83/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0058 - val_accuracy: 0.9983\n",
            "Epoch 84/100\n",
            "67/67 [==============================] - 2s 35ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0090 - val_accuracy: 0.9966\n",
            "Epoch 85/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.0081 - val_accuracy: 0.9966\n",
            "Epoch 86/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9983\n",
            "Epoch 87/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 0.9974\n",
            "Epoch 88/100\n",
            "67/67 [==============================] - 2s 26ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0062 - val_accuracy: 0.9974\n",
            "Epoch 89/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.5832 - val_accuracy: 0.8771\n",
            "Epoch 90/100\n",
            "67/67 [==============================] - 2s 31ms/step - loss: 0.0317 - accuracy: 0.9904 - val_loss: 0.0073 - val_accuracy: 0.9983\n",
            "Epoch 91/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0072 - val_accuracy: 0.9966\n",
            "Epoch 92/100\n",
            "67/67 [==============================] - 2s 33ms/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.0338 - val_accuracy: 0.9906\n",
            "Epoch 93/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0152 - accuracy: 0.9956 - val_loss: 0.0043 - val_accuracy: 0.9983\n",
            "Epoch 94/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0072 - val_accuracy: 0.9974\n",
            "Epoch 95/100\n",
            "67/67 [==============================] - 2s 24ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0032 - val_accuracy: 0.9991\n",
            "Epoch 96/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0293 - val_accuracy: 0.9923\n",
            "Epoch 97/100\n",
            "67/67 [==============================] - 2s 25ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.0036 - val_accuracy: 0.9991\n",
            "Epoch 98/100\n",
            "67/67 [==============================] - 2s 29ms/step - loss: 0.0301 - accuracy: 0.9898 - val_loss: 0.0158 - val_accuracy: 0.9940\n",
            "Epoch 99/100\n",
            "67/67 [==============================] - 3s 38ms/step - loss: 0.0160 - accuracy: 0.9956 - val_loss: 0.0038 - val_accuracy: 0.9991\n",
            "Epoch 100/100\n",
            "67/67 [==============================] - 2s 34ms/step - loss: 0.0136 - accuracy: 0.9967 - val_loss: 0.0084 - val_accuracy: 0.9957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The model with dropout = 0.2 performed better again, so we will keep that model and test it.\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2byikbG6YQq1",
        "outputId": "9b099d83-0b94-468b-ea97-65aaceb73121"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03151794150471687, 0.992017388343811]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimately, the neural network only outperformed the random forest by .05% in its ability to classify physical human activities."
      ],
      "metadata": {
        "id": "U40cT6fKYygU"
      }
    }
  ]
}